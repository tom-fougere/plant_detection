{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import random\n",
    "from shutil import copyfile\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "HEIGHT = 64\n",
    "WIDTH = 64\n",
    "BATCH_SIZE = 32\n",
    "SPLIT_RATIO = 0.8\n",
    "TARGET_SIZE = (HEIGHT, WIDTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unzip file\n",
    "import tarfile\n",
    "\n",
    "filename = 'dataset/synthetic_sugarbeet_random_weeds'\n",
    "\n",
    "# my_tar = tarfile.open(filename + '.tar.gz')\n",
    "# my_tar.extractall('dataset')\n",
    "# my_tar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create all directories from a string path\n",
    "def make_directory(fullpath):\n",
    "    splitted_data = fullpath.split('/')\n",
    "    \n",
    "    new_dir = []\n",
    "    current_dir = '.'\n",
    "    \n",
    "    for folder in splitted_data:           \n",
    "        current_dir = current_dir + '/' + folder                \n",
    "        if not os.path.isdir(current_dir + '/'):\n",
    "            new_dir.append(current_dir)\n",
    "            os.mkdir(current_dir)\n",
    "    \n",
    "    return new_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a python function called split_data which takes\n",
    "# a FOLDER_IMAGES directory containing the images\n",
    "# a FOLDER_MASKS directory containing the masks\n",
    "# a TARGET_FOLDER directory where the files will be copied to\n",
    "# the TARGET_FOLDER directory will contain 2 subfolders train and test with the data splitted\n",
    "# a SPLIT SIZE to determine the portion\n",
    "# The files should also be randomized, so that the training set is a random\n",
    "# X% of the files, and the test set is the remaining files\n",
    "# SO, for example, if SPLIT SIZE is .9\n",
    "# Then 90% of the images will be copied to the TARGET_FOLDER/train dir\n",
    "# and 10% of the images will be copied to the TARGET_FOLDER/test dir\n",
    "# Also -- All images should be checked, and if they have a zero file length,\n",
    "# they will not be copied over\n",
    "def split_data(FOLDER_IMAGES, FOLDER_MASKS, TARGET_FOLDER, SPLIT_SIZE):\n",
    "\n",
    "    TRAINING_FOLDER_NAME = TARGET_FOLDER + '/train/'\n",
    "    TESTING_FOLDER_NAME = TARGET_FOLDER + '/test/'\n",
    "    TRAINING_FOLDER_NAME_IMAGES = TARGET_FOLDER + '/train/images/img/'\n",
    "    TRAINING_FOLDER_NAME_MASKS = TARGET_FOLDER + '/train/masks/img/'\n",
    "    TESTING_FOLDER_NAME_IMAGES = TARGET_FOLDER + '/test/images/img/'\n",
    "    TESTING_FOLDER_NAME_MASKS = TARGET_FOLDER + '/test/masks/img/'\n",
    "\n",
    "    # Create directories\n",
    "    make_directory(TRAINING_FOLDER_NAME_IMAGES)\n",
    "    make_directory(TRAINING_FOLDER_NAME_MASKS)\n",
    "    make_directory(TESTING_FOLDER_NAME_IMAGES)\n",
    "    make_directory(TESTING_FOLDER_NAME_MASKS)\n",
    "    \n",
    "    # Remove all data in TRAINING and TESTING dir\n",
    "    for i_file in os.listdir(TRAINING_FOLDER_NAME_IMAGES):\n",
    "        os.remove(TRAINING_FOLDER_NAME_IMAGES + i_file)\n",
    "    for i_file in os.listdir(TRAINING_FOLDER_NAME_MASKS):\n",
    "        os.remove(TRAINING_FOLDER_NAME_MASKS + i_file)\n",
    "    for i_file in os.listdir(TESTING_FOLDER_NAME_IMAGES):\n",
    "        os.remove(TESTING_FOLDER_NAME_IMAGES + i_file)\n",
    "    for i_file in os.listdir(TESTING_FOLDER_NAME_MASKS):\n",
    "        os.remove(TESTING_FOLDER_NAME_MASKS + i_file)\n",
    "    \n",
    "    dataset = []\n",
    "    \n",
    "    # Check for z zero file length\n",
    "    for i_file in os.listdir(FOLDER_IMAGES):\n",
    "        data = i_file\n",
    "        if (os.path.getsize(FOLDER_IMAGES + data) > 0):\n",
    "            dataset.append(i_file)\n",
    "        else:\n",
    "            print('Skipped ' + i_file)\n",
    "            print('Invalid file size! i.e Zero length.')\n",
    "    \n",
    "    # Number of files\n",
    "    nb_files = len(dataset)\n",
    "    nb_files_training = int(nb_files * SPLIT_SIZE)\n",
    "    nb_files_testing = nb_files - nb_files_training\n",
    "    \n",
    "    # Suffle dataset\n",
    "    shuffled_dataset = random.sample(dataset, len(dataset))\n",
    "        \n",
    "    # Copy files\n",
    "    for i_num, i_file in enumerate(shuffled_dataset):\n",
    "        if i_num < nb_files_training:\n",
    "            new_path_images = TRAINING_FOLDER_NAME_IMAGES + i_file\n",
    "            new_path_masks = TRAINING_FOLDER_NAME_MASKS + i_file\n",
    "        else:\n",
    "            new_path_images = TESTING_FOLDER_NAME_IMAGES + i_file\n",
    "            new_path_masks = TESTING_FOLDER_NAME_MASKS + i_file\n",
    "            \n",
    "        copyfile(FOLDER_IMAGES + i_file, new_path_images)\n",
    "        copyfile(FOLDER_MASKS + i_file, new_path_masks)\n",
    "        \n",
    "    return nb_files_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trained images: 1001\n"
     ]
    }
   ],
   "source": [
    "# Split data\n",
    "FOLDER_IMAGES = \"dataset/synthetic_sugarbeet_random_weeds/rgb/\"\n",
    "FOLDER_MASKS = \"dataset/synthetic_sugarbeet_random_weeds/gt/\"\n",
    "TARGET_FOLDER = \"dataset/synthetic_sugarbeet_random_weeds/train_test/\"\n",
    "\n",
    "NB_TRAINED_IMAGES = split_data(FOLDER_IMAGES, FOLDER_MASKS, TARGET_FOLDER, SPLIT_RATIO)\n",
    "print('Number of trained images:', NB_TRAINED_IMAGES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0.0, 1.0, 2.0}\n",
      "{0.0, 1.0, 2.0}\n",
      "{0.0, 1.0, 2.0}\n"
     ]
    }
   ],
   "source": [
    "# Understand how the mask is made\n",
    "mask_path = \"dataset/synthetic_sugarbeet_random_weeds/gt/\"\n",
    "list_masks = os.listdir(mask_path)\n",
    "\n",
    "masks_layer1 = []\n",
    "masks_layer2 = []\n",
    "masks_layer3 = []\n",
    "for i in range(5):\n",
    "    current_mask = cv2.imread(mask_path + list_masks[i])\n",
    "    masks_layer1 = np.concatenate([masks_layer1, current_mask[:,:,0].flatten()])\n",
    "    masks_layer2 = np.concatenate([masks_layer1, current_mask[:,:,1].flatten()])\n",
    "    masks_layer3 = np.concatenate([masks_layer1, current_mask[:,:,2].flatten()])\n",
    "    \n",
    "print(set(masks_layer1))\n",
    "print(set(masks_layer2))\n",
    "print(set(masks_layer3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 372,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Plot masks\n",
    "mask_path = \"dataset/synthetic_sugarbeet_random_weeds/gt/\"\n",
    "list_masks = os.listdir(mask_path)\n",
    "\n",
    "for i in range(3):\n",
    "    current_mask = cv2.imread(mask_path + list_masks[i])\n",
    "    current_mask = current_mask*100\n",
    "    layers = np.concatenate((current_mask[:,:,0], current_mask[:,:,1], current_mask[:,:,2]), axis=1)\n",
    "    \n",
    "    if i > 0:\n",
    "        images = np.concatenate((images, layers), axis=0)\n",
    "    else:\n",
    "        images = layers            \n",
    "\n",
    "# Plot image\n",
    "cv2.imshow('Example', images) \n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-processing of images and masks\n",
    "def preprocessing_masks(image):\n",
    "    img = image>0\n",
    "    img = tf.cast(img, tf.uint8)\n",
    "    return img\n",
    "\n",
    "def preprocessing_images(image):\n",
    "    img = tf.cast(image, tf.float32)\n",
    "    img = tf.image.resize(img, TARGET_SIZE)\n",
    "    img = img / 255.\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Post processing of images and masks for visualization\n",
    "def postprocessing_masks(image, new_size=TARGET_SIZE):\n",
    "    img = image>0\n",
    "    img = img * 255\n",
    "    img = img.astype(np.uint8)\n",
    "    img = cv2.resize(img, (new_size[1], new_size[0]))\n",
    "    return img\n",
    "\n",
    "def postprocessing_images(image, new_size=TARGET_SIZE):\n",
    "    img = cv2.resize(image, (new_size[1], new_size[0]))\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot an image with its mask\n",
    "def plot_image_with_mask(image, mask):    \n",
    "    mask_norm = postprocessing_masks(mask, image.shape)\n",
    "        \n",
    "    mixed_image = cv2.addWeighted(image, 0.5, mask_norm, 0.5, 0)      \n",
    "    raw_images = np.concatenate((image, mask_norm), axis=1)\n",
    "    example = np.concatenate((raw_images, mixed_image), axis=1)\n",
    "    \n",
    "    # Plot image\n",
    "    cv2.imshow('Example', example) \n",
    "    cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images: 1001\n",
      "Selected image: 1\n",
      "PP uint8\n"
     ]
    }
   ],
   "source": [
    "list_files = os.listdir(TARGET_FOLDER + 'train/images/img/')\n",
    "print('Number of images:', len(list_files))\n",
    "IMG_ID = 1\n",
    "print('Selected image:', IMG_ID)\n",
    "\n",
    "example_image = cv2.imread(TARGET_FOLDER + 'train/images/img/' + list_files[IMG_ID])\n",
    "example_mask = cv2.imread(TARGET_FOLDER + 'train/masks/img/' + list_files[IMG_ID])\n",
    "\n",
    "plot_image_with_mask(example_image, example_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1001 images belonging to 1 classes.\n",
      "Found 1001 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "# Data augmentation (for train dataset)\n",
    "SEED = 1\n",
    "\n",
    "train_image_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale = 1/255.,\n",
    "                                                                horizontal_flip=True,\n",
    "                                                                rotation_range=20,\n",
    "                                                                zoom_range=0.2,\n",
    "                                                                width_shift_range = 0.1,\n",
    "                                                                height_shift_range = 0.1)\n",
    "train_image_generator= train_image_datagen.flow_from_directory(TARGET_FOLDER + 'train/images',\n",
    "                                                               batch_size=BATCH_SIZE,\n",
    "                                                               target_size=TARGET_SIZE,\n",
    "                                                               class_mode=None,\n",
    "                                                               seed=SEED)\n",
    "\n",
    "train_mask_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "                                                                horizontal_flip=True,\n",
    "                                                                rotation_range=20,\n",
    "                                                                zoom_range=0.2,\n",
    "                                                                width_shift_range = 0.1,\n",
    "                                                                height_shift_range = 0.1,\n",
    "                                                                preprocessing_function=preprocessing_masks)\n",
    "train_mask_generator= train_image_datagen.flow_from_directory(TARGET_FOLDER + 'train/masks/',\n",
    "                                                               batch_size=BATCH_SIZE,\n",
    "                                                               target_size=TARGET_SIZE,\n",
    "                                                               class_mode=None,\n",
    "                                                               seed=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 251 images belonging to 1 classes.\n",
      "Found 251 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "# Data augmentation (for validation dataset)\n",
    "SEED = 1\n",
    "\n",
    "val_image_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale = 1/255.)\n",
    "val_image_generator= val_image_datagen.flow_from_directory(TARGET_FOLDER + 'test/images',\n",
    "                                                               batch_size=BATCH_SIZE,\n",
    "                                                               target_size=TARGET_SIZE,\n",
    "                                                               class_mode=None,\n",
    "                                                               seed=SEED)\n",
    "\n",
    "val_mask_datagen = tf.keras.preprocessing.image.ImageDataGenerator(preprocessing_function=preprocessing_masks)\n",
    "val_mask_generator= val_image_datagen.flow_from_directory(TARGET_FOLDER + 'test/masks/',\n",
    "                                                               batch_size=BATCH_SIZE,\n",
    "                                                               target_size=TARGET_SIZE,\n",
    "                                                               class_mode=None,\n",
    "                                                               seed=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create generator\n",
    "def my_image_mask_generator(image_data_generator, mask_data_generator):\n",
    "    new_generator = zip(image_data_generator, mask_data_generator)\n",
    "    for (img, mask) in new_generator:\n",
    "        yield (img, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameter describing where the channel dimension is found in our dataset\n",
    "IMAGE_ORDERING = 'channels_last'\n",
    "\n",
    "def conv_block(input, filters, strides, pooling_size, pool_strides):\n",
    "  '''\n",
    "  Args:\n",
    "    input (tensor) -- batch of images or features\n",
    "    filters (int) -- number of filters of the Conv2D layers\n",
    "    strides (int) -- strides setting of the Conv2D layers\n",
    "    pooling_size (int) -- pooling size of the MaxPooling2D layers\n",
    "    pool_strides (int) -- strides setting of the MaxPooling2D layers\n",
    "  \n",
    "  Returns:\n",
    "    (tensor) max pooled and batch-normalized features of the input \n",
    "  '''\n",
    "  # use the functional syntax to stack the layers as shown in the diagram above\n",
    "  x = tf.keras.layers.Conv2D(filters, strides, padding='same', data_format=IMAGE_ORDERING)(input)\n",
    "  x = tf.keras.layers.LeakyReLU()(x)\n",
    "  x = tf.keras.layers.Conv2D(filters, strides, padding='same')(x)\n",
    "  x = tf.keras.layers.LeakyReLU()(x)\n",
    "  x = tf.keras.layers.MaxPooling2D(pool_size=pooling_size, strides=pool_strides)(x)\n",
    "  x = tf.keras.layers.BatchNormalization()(x)\n",
    "\n",
    "  return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_9 (InputLayer)         [(None, 64, 64, 3)]       0         \n",
      "_________________________________________________________________\n",
      "conv2d_76 (Conv2D)           (None, 64, 64, 32)        896       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_64 (LeakyReLU)   (None, 64, 64, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_77 (Conv2D)           (None, 64, 64, 32)        9248      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_65 (LeakyReLU)   (None, 64, 64, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_32 (MaxPooling (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_32 (Batc (None, 32, 32, 32)        128       \n",
      "=================================================================\n",
      "Total params: 10,272\n",
      "Trainable params: 10,208\n",
      "Non-trainable params: 64\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# TEST CODE:\n",
    "\n",
    "test_input = tf.keras.layers.Input(shape=(HEIGHT, WIDTH, 3))\n",
    "test_output = conv_block(test_input, 32, 3, 2, 2)\n",
    "test_model = tf.keras.Model(inputs=test_input, outputs=test_output)\n",
    "\n",
    "print(test_model.summary())\n",
    "\n",
    "# free up test resources\n",
    "del test_input, test_output, test_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FCN8(input_height=HEIGHT, input_width=WIDTH):\n",
    "    '''\n",
    "    Defines the downsampling path of the image segmentation model.\n",
    "\n",
    "    Args:\n",
    "      input_height (int) -- height of the images\n",
    "      width (int) -- width of the images\n",
    "\n",
    "    Returns:\n",
    "    (tuple of tensors, tensor)\n",
    "      tuple of tensors -- features extracted at blocks 3 to 5\n",
    "      tensor -- copy of the input\n",
    "    '''\n",
    "   \n",
    "    img_input = tf.keras.layers.Input(shape=(input_height,input_width, 3))\n",
    "    \n",
    "    # pad the input image to have dimensions to the nearest power of two\n",
    "    x = tf.keras.layers.ZeroPadding2D(padding=(0, 0))(img_input)\n",
    "\n",
    "    # Block 1\n",
    "    x = conv_block(x, filters=32, strides=(3, 3), pooling_size=(2, 2), pool_strides=(2, 2))\n",
    "    \n",
    "    # Block 2\n",
    "    x = conv_block(x, filters=64, strides=(3, 3), pooling_size=(2, 2), pool_strides=(2, 2))\n",
    "\n",
    "    # Block 3\n",
    "    x = conv_block(x, filters=128, strides=(3, 3), pooling_size=(2, 2), pool_strides=(2, 2))\n",
    "    # save the feature map at this stage\n",
    "    f3 = x\n",
    "\n",
    "    # Block 4\n",
    "    x = conv_block(x, filters=256, strides=(3, 3), pooling_size=(2, 2), pool_strides=(2, 2))\n",
    "    # save the feature map at this stage\n",
    "    f4 = x\n",
    "\n",
    "    # Block 5\n",
    "    x = conv_block(x, filters=256, strides=(3, 3), pooling_size=(2, 2), pool_strides=(2, 2))\n",
    "    # save the feature map at this stage\n",
    "    f5 = x\n",
    "  \n",
    "    return (f3, f4, f5), img_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_10 (InputLayer)        [(None, 64, 64, 3)]       0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_6 (ZeroPaddin (None, 64, 64, 3)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_78 (Conv2D)           (None, 64, 64, 32)        896       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_66 (LeakyReLU)   (None, 64, 64, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_79 (Conv2D)           (None, 64, 64, 32)        9248      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_67 (LeakyReLU)   (None, 64, 64, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_33 (MaxPooling (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_33 (Batc (None, 32, 32, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_80 (Conv2D)           (None, 32, 32, 64)        18496     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_68 (LeakyReLU)   (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_81 (Conv2D)           (None, 32, 32, 64)        36928     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_69 (LeakyReLU)   (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_34 (MaxPooling (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_34 (Batc (None, 16, 16, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_82 (Conv2D)           (None, 16, 16, 128)       73856     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_70 (LeakyReLU)   (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_83 (Conv2D)           (None, 16, 16, 128)       147584    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_71 (LeakyReLU)   (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_35 (MaxPooling (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_35 (Batc (None, 8, 8, 128)         512       \n",
      "_________________________________________________________________\n",
      "conv2d_84 (Conv2D)           (None, 8, 8, 256)         295168    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_72 (LeakyReLU)   (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_85 (Conv2D)           (None, 8, 8, 256)         590080    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_73 (LeakyReLU)   (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_36 (MaxPooling (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_36 (Batc (None, 4, 4, 256)         1024      \n",
      "_________________________________________________________________\n",
      "conv2d_86 (Conv2D)           (None, 4, 4, 256)         590080    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_74 (LeakyReLU)   (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_87 (Conv2D)           (None, 4, 4, 256)         590080    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_75 (LeakyReLU)   (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_37 (MaxPooling (None, 2, 2, 256)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_37 (Batc (None, 2, 2, 256)         1024      \n",
      "=================================================================\n",
      "Total params: 2,355,360\n",
      "Trainable params: 2,353,888\n",
      "Non-trainable params: 1,472\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# TEST CODE:\n",
    "test_convs, test_img_input = FCN8()\n",
    "test_model = tf.keras.Model(inputs=test_img_input, outputs=[test_convs, test_img_input])\n",
    "\n",
    "print(test_model.summary())\n",
    "\n",
    "del test_convs, test_img_input, test_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fcn8_decoder(convs, n_classes):\n",
    "  # features from the encoder stage\n",
    "  f3, f4, f5 = convs\n",
    "\n",
    "  # number of filters\n",
    "  n = 512\n",
    "\n",
    "  # add convolutional layers on top of the CNN extractor.\n",
    "  o = tf.keras.layers.Conv2D(n , (7 , 7) , activation='relu' , padding='same', name=\"conv6\", data_format=IMAGE_ORDERING)(f5)\n",
    "  o = tf.keras.layers.Dropout(0.5)(o)\n",
    "\n",
    "  o = tf.keras.layers.Conv2D(n , (1 , 1) , activation='relu' , padding='same', name=\"conv7\", data_format=IMAGE_ORDERING)(o)\n",
    "  o = tf.keras.layers.Dropout(0.5)(o)\n",
    "\n",
    "  o = tf.keras.layers.Conv2D(n_classes,  (1, 1), activation='relu' , padding='same', data_format=IMAGE_ORDERING)(o)\n",
    "\n",
    "\n",
    "  # Upsample `o` above and crop any extra pixels introduced\n",
    "  o = tf.keras.layers.Conv2DTranspose(n_classes , kernel_size=(4,4) ,  strides=(2,2) , use_bias=False )(f5)\n",
    "  o = tf.keras.layers.Cropping2D(cropping=(1,1))(o)\n",
    "\n",
    "  # load the pool 4 prediction and do a 1x1 convolution to reshape it to the same shape of `o` above\n",
    "  o2 = f4\n",
    "  o2 = tf.keras.layers.Conv2D(n_classes, kernel_size=(1, 1), activation='relu', padding='same', data_format=IMAGE_ORDERING)(o2)\n",
    "\n",
    "  # add the results of the upsampling and pool 4 prediction\n",
    "  o = tf.keras.layers.Add()([o, o2])\n",
    "\n",
    "  # upsample the resulting tensor of the operation you just did\n",
    "  o = tf.keras.layers.Conv2DTranspose(n_classes , kernel_size=(4,4) ,  strides=(2,2) , use_bias=False )(o)\n",
    "  o = tf.keras.layers.Cropping2D(cropping=(1,1))(o)\n",
    "\n",
    "  # load the pool 3 prediction and do a 1x1 convolution to reshape it to the same shape of `o` above\n",
    "  o2 = f3\n",
    "  o2 = tf.keras.layers.Conv2D(n_classes , ( 1 , 1 ) , activation='relu' , padding='same', data_format=IMAGE_ORDERING)(o2)\n",
    "\n",
    "  # add the results of the upsampling and pool 3 prediction\n",
    "  o = tf.keras.layers.Add()([o, o2])\n",
    "\n",
    "  # upsample up to the size of the original image\n",
    "  o = tf.keras.layers.Conv2DTranspose(n_classes , kernel_size=(8,8) ,  strides=(8,8) , use_bias=False )(o)\n",
    "  o = tf.keras.layers.Cropping2D(((0, HEIGHT-HEIGHT), (0, 0)))(o)\n",
    "\n",
    "  # append a sigmoid activation\n",
    "  o = (tf.keras.layers.Activation('sigmoid'))(o)\n",
    "\n",
    "  return o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 64, 64, 1)\n"
     ]
    }
   ],
   "source": [
    "# TEST CODE:\n",
    "test_convs, test_img_input = FCN8()\n",
    "test_fcn8_decoder = fcn8_decoder(test_convs, 1)\n",
    "\n",
    "print(test_fcn8_decoder.shape)\n",
    "\n",
    "del test_convs, test_img_input, test_fcn8_decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 64, 64, 3)\n"
     ]
    }
   ],
   "source": [
    "# start the encoder using the default input size 64 x 84\n",
    "convs, img_input = FCN8()\n",
    "\n",
    "# pass the convolutions obtained in the encoder to the decoder\n",
    "n_classes = 3\n",
    "dec_op = fcn8_decoder(convs, n_classes)\n",
    "\n",
    "print(dec_op.shape)\n",
    "\n",
    "# define the model specifying the input (batch of images) and output (decoder output)\n",
    "model = tf.keras.Model(inputs = img_input, outputs = dec_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 31.28125 steps, validate for 31.28125 steps\n",
      "Epoch 1/30\n",
      "32/31 [==============================] - 31s 978ms/step - loss: 0.0015 - accuracy: 0.9452 - val_loss: 0.0016 - val_accuracy: 0.9606\n",
      "Epoch 2/30\n",
      "32/31 [==============================] - 29s 904ms/step - loss: 0.0015 - accuracy: 0.9486 - val_loss: 0.0016 - val_accuracy: 0.9610\n",
      "Epoch 3/30\n",
      "32/31 [==============================] - 28s 890ms/step - loss: 0.0015 - accuracy: 0.9458 - val_loss: 0.0016 - val_accuracy: 0.9601\n",
      "Epoch 4/30\n",
      "32/31 [==============================] - 34s 1s/step - loss: 0.0015 - accuracy: 0.9476 - val_loss: 0.0016 - val_accuracy: 0.9606\n",
      "Epoch 5/30\n",
      "32/31 [==============================] - 33s 1s/step - loss: 0.0015 - accuracy: 0.9472 - val_loss: 0.0015 - val_accuracy: 0.9608\n",
      "Epoch 6/30\n",
      "32/31 [==============================] - 32s 1s/step - loss: 0.0016 - accuracy: 0.9451 - val_loss: 0.0016 - val_accuracy: 0.9608\n",
      "Epoch 7/30\n",
      "32/31 [==============================] - 33s 1s/step - loss: 0.0015 - accuracy: 0.9463 - val_loss: 0.0017 - val_accuracy: 0.9610\n",
      "Epoch 8/30\n",
      "32/31 [==============================] - 33s 1s/step - loss: 0.0015 - accuracy: 0.9466 - val_loss: 0.0017 - val_accuracy: 0.9608\n",
      "Epoch 9/30\n",
      "32/31 [==============================] - 30s 929ms/step - loss: 0.0015 - accuracy: 0.9491 - val_loss: 0.0019 - val_accuracy: 0.9608\n",
      "Epoch 10/30\n",
      "32/31 [==============================] - 29s 915ms/step - loss: 0.0016 - accuracy: 0.9448 - val_loss: 0.0016 - val_accuracy: 0.9606\n",
      "Epoch 11/30\n",
      "32/31 [==============================] - 28s 877ms/step - loss: 0.0015 - accuracy: 0.9479 - val_loss: 0.0015 - val_accuracy: 0.9614\n",
      "Epoch 12/30\n",
      "32/31 [==============================] - 29s 893ms/step - loss: 0.0015 - accuracy: 0.9475 - val_loss: 0.0017 - val_accuracy: 0.9610\n",
      "Epoch 13/30\n",
      "32/31 [==============================] - 28s 883ms/step - loss: 0.0015 - accuracy: 0.9473 - val_loss: 0.0014 - val_accuracy: 0.9615\n",
      "Epoch 14/30\n",
      "32/31 [==============================] - 30s 930ms/step - loss: 0.0014 - accuracy: 0.9485 - val_loss: 0.0015 - val_accuracy: 0.9610\n",
      "Epoch 15/30\n",
      "32/31 [==============================] - 32s 998ms/step - loss: 0.0016 - accuracy: 0.9461 - val_loss: 0.0014 - val_accuracy: 0.9610\n",
      "Epoch 16/30\n",
      "32/31 [==============================] - 28s 890ms/step - loss: 0.0016 - accuracy: 0.9448 - val_loss: 0.0015 - val_accuracy: 0.9608\n",
      "Epoch 17/30\n",
      "32/31 [==============================] - 29s 903ms/step - loss: 0.0015 - accuracy: 0.9470 - val_loss: 0.0015 - val_accuracy: 0.9613\n",
      "Epoch 18/30\n",
      "32/31 [==============================] - 29s 901ms/step - loss: 0.0015 - accuracy: 0.9483 - val_loss: 0.0014 - val_accuracy: 0.9614\n",
      "Epoch 19/30\n",
      "32/31 [==============================] - 28s 881ms/step - loss: 0.0015 - accuracy: 0.9448 - val_loss: 0.0014 - val_accuracy: 0.9608\n",
      "Epoch 20/30\n",
      "32/31 [==============================] - 29s 910ms/step - loss: 0.0015 - accuracy: 0.9450 - val_loss: 0.0014 - val_accuracy: 0.9617\n",
      "Epoch 21/30\n",
      "32/31 [==============================] - 30s 928ms/step - loss: 0.0016 - accuracy: 0.9460 - val_loss: 0.0016 - val_accuracy: 0.9612\n",
      "Epoch 22/30\n",
      "32/31 [==============================] - 28s 881ms/step - loss: 0.0015 - accuracy: 0.9481 - val_loss: 0.0015 - val_accuracy: 0.9612\n",
      "Epoch 23/30\n",
      "32/31 [==============================] - 29s 895ms/step - loss: 0.0015 - accuracy: 0.9468 - val_loss: 0.0014 - val_accuracy: 0.9610\n",
      "Epoch 24/30\n",
      "32/31 [==============================] - 28s 884ms/step - loss: 0.0016 - accuracy: 0.9445 - val_loss: 0.0014 - val_accuracy: 0.9609\n",
      "Epoch 25/30\n",
      "32/31 [==============================] - 28s 878ms/step - loss: 0.0015 - accuracy: 0.9475 - val_loss: 0.0014 - val_accuracy: 0.9606\n",
      "Epoch 26/30\n",
      "32/31 [==============================] - 30s 950ms/step - loss: 0.0015 - accuracy: 0.9461 - val_loss: 0.0015 - val_accuracy: 0.9602\n",
      "Epoch 27/30\n",
      "32/31 [==============================] - 33s 1s/step - loss: 0.0015 - accuracy: 0.9469 - val_loss: 0.0014 - val_accuracy: 0.9609\n",
      "Epoch 28/30\n",
      "32/31 [==============================] - 29s 913ms/step - loss: 0.0015 - accuracy: 0.9479 - val_loss: 0.0014 - val_accuracy: 0.9618\n",
      "Epoch 29/30\n",
      "32/31 [==============================] - 33s 1s/step - loss: 0.0015 - accuracy: 0.9459 - val_loss: 0.0014 - val_accuracy: 0.9610\n",
      "Epoch 30/30\n",
      "31/31 [============================>.] - ETA: 0s - loss: 0.0015 - accuracy: 0.9457"
     ]
    }
   ],
   "source": [
    "# Create custom generator for training images and masks\n",
    "# my_train_generator = my_image_mask_generator(train_image_generator, train_mask_generator)\n",
    "# my_val_generator = my_image_mask_generator(val_image_generator, val_mask_generator)\n",
    "\n",
    "my_train_generator = zip(train_image_generator, train_mask_generator)\n",
    "my_val_generator = zip(val_image_generator, val_mask_generator)\n",
    "\n",
    "# Compile your model here\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "steps = NB_TRAINED_IMAGES / BATCH_SIZE\n",
    "\n",
    "# Train your model here\n",
    "history = model.fit(my_train_generator,\n",
    "                    steps_per_epoch=steps,\n",
    "                    epochs=30,\n",
    "                    verbose=1,\n",
    "                    validation_data=my_val_generator,\n",
    "                    validation_steps=steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 415,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Plot prediction\n",
    "IMG_ID = 1\n",
    "pathname = 'train/images/img/'\n",
    "list_files = os.listdir(TARGET_FOLDER + pathname)\n",
    "\n",
    "example_image = cv2.imread(TARGET_FOLDER + 'train/images/img/' + list_files[IMG_ID])\n",
    "example_mask = cv2.imread(TARGET_FOLDER + 'train/masks/img/' + list_files[IMG_ID])\n",
    "\n",
    "image_processed = preprocessing_images(example_image)\n",
    "image_processed = tf.expand_dims(image_processed, axis=0)\n",
    "\n",
    "prediction = model.predict(image_processed)\n",
    "pred_image = np.array(tf.squeeze(prediction, axis=0))\n",
    "\n",
    "image_example_gt = np.concatenate((postprocessing_images(example_image),\n",
    "                                   postprocessing_masks(example_mask),\n",
    "                                   pred_image), axis=1)\n",
    "cv2.imshow('Example', image_example_gt) \n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.067642\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "flatten_image = pred_image.flatten()\n",
    "print(max(flatten_image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
