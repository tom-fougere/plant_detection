{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import random\n",
    "from shutil import copyfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "HEIGHT =64\n",
    "WIDTH = 64\n",
    "TARGET_SIZE = (HEIGHT, WIDTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unzip file\n",
    "import tarfile\n",
    "\n",
    "filename = 'dataset/synthetic_sugarbeet_random_weeds'\n",
    "\n",
    "# my_tar = tarfile.open(filename + '.tar.gz')\n",
    "# my_tar.extractall('dataset')\n",
    "# my_tar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "def make_directory(fullpath):\n",
    "    splitted_data = fullpath.split('/')\n",
    "    \n",
    "    new_dir = []\n",
    "    current_dir = '.'\n",
    "    \n",
    "    for folder in splitted_data:           \n",
    "        current_dir = current_dir + '/' + folder                \n",
    "        if not os.path.isdir(current_dir + '/'):\n",
    "            new_dir.append(current_dir)\n",
    "            os.mkdir(current_dir)\n",
    "    \n",
    "    return new_dir\n",
    "    \n",
    "print(make_directory('dataset/synthetic_sugarbeet_random_weeds/tom/fougere/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a python function called split_data which takes\n",
    "# a FOLDER_IMAGES directory containing the images\n",
    "# a FOLDER_MASKS directory containing the masks\n",
    "# a TARGET_FOLDER directory where the files will be copied to\n",
    "# the TARGET_FOLDER directory will contain 2 subfolders train and test with the data splitted\n",
    "# a SPLIT SIZE to determine the portion\n",
    "# The files should also be randomized, so that the training set is a random\n",
    "# X% of the files, and the test set is the remaining files\n",
    "# SO, for example, if SPLIT SIZE is .9\n",
    "# Then 90% of the images will be copied to the TRAINING dir\n",
    "# and 10% of the images will be copied to the TESTING dir\n",
    "# Also -- All images should be checked, and if they have a zero file length,\n",
    "# they will not be copied over\n",
    "def split_data(FOLDER_IMAGES, FOLDER_MASKS, TARGET_FOLDER, SPLIT_SIZE):\n",
    "\n",
    "    TRAINING_FOLDER_NAME = TARGET_FOLDER + '/train/'\n",
    "    TESTING_FOLDER_NAME = TARGET_FOLDER + '/test/'\n",
    "    TRAINING_FOLDER_NAME_IMAGES = TARGET_FOLDER + '/train/images/img/'\n",
    "    TRAINING_FOLDER_NAME_MASKS = TARGET_FOLDER + '/train/masks/img/'\n",
    "    TESTING_FOLDER_NAME_IMAGES = TARGET_FOLDER + '/test/images/img/'\n",
    "    TESTING_FOLDER_NAME_MASKS = TARGET_FOLDER + '/test/masks/img/'\n",
    "\n",
    "    # Create directories\n",
    "    make_directory(TRAINING_FOLDER_NAME_IMAGES)\n",
    "    make_directory(TRAINING_FOLDER_NAME_MASKS)\n",
    "    make_directory(TESTING_FOLDER_NAME_IMAGES)\n",
    "    make_directory(TESTING_FOLDER_NAME_MASKS)\n",
    "    \n",
    "    # Remove all data in TRAINING and TESTING dir\n",
    "    for i_file in os.listdir(TRAINING_FOLDER_NAME_IMAGES):\n",
    "        os.remove(TRAINING_FOLDER_NAME_IMAGES + i_file)\n",
    "    for i_file in os.listdir(TRAINING_FOLDER_NAME_MASKS):\n",
    "        os.remove(TRAINING_FOLDER_NAME_MASKS + i_file)\n",
    "    for i_file in os.listdir(TESTING_FOLDER_NAME_IMAGES):\n",
    "        os.remove(TESTING_FOLDER_NAME_IMAGES + i_file)\n",
    "    for i_file in os.listdir(TESTING_FOLDER_NAME_MASKS):\n",
    "        os.remove(TESTING_FOLDER_NAME_MASKS + i_file)\n",
    "    \n",
    "    dataset = []\n",
    "    \n",
    "    # Check for z zero file length\n",
    "    for i_file in os.listdir(FOLDER_IMAGES):\n",
    "        data = i_file\n",
    "        if (os.path.getsize(FOLDER_IMAGES + data) > 0):\n",
    "            dataset.append(i_file)\n",
    "        else:\n",
    "            print('Skipped ' + i_file)\n",
    "            print('Invalid file size! i.e Zero length.')\n",
    "    \n",
    "    # Number of files\n",
    "    nb_files = len(dataset)\n",
    "    nb_files_training = int(nb_files * SPLIT_SIZE)\n",
    "    nb_files_testing = nb_files - nb_files_training\n",
    "    \n",
    "    # Suffle dataset\n",
    "    shuffled_dataset = random.sample(dataset, len(dataset))\n",
    "        \n",
    "    # Copy files\n",
    "    for i_num, i_file in enumerate(shuffled_dataset):\n",
    "        if i_num < nb_files_training:\n",
    "            new_path_images = TRAINING_FOLDER_NAME_IMAGES + i_file\n",
    "            new_path_masks = TRAINING_FOLDER_NAME_MASKS + i_file\n",
    "        else:\n",
    "            new_path_images = TESTING_FOLDER_NAME_IMAGES + i_file\n",
    "            new_path_masks = TESTING_FOLDER_NAME_MASKS + i_file\n",
    "            \n",
    "        copyfile(FOLDER_IMAGES + i_file, new_path_images)\n",
    "        copyfile(FOLDER_MASKS + i_file, new_path_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "FOLDER_IMAGES = \"dataset/synthetic_sugarbeet_random_weeds/rgb/\"\n",
    "FOLDER_MASKS = \"dataset/synthetic_sugarbeet_random_weeds/gt/\"\n",
    "TARGET_FOLDER = \"dataset/synthetic_sugarbeet_random_weeds/train_test/\"\n",
    "\n",
    "split_size = .9\n",
    "split_data(FOLDER_IMAGES, FOLDER_MASKS, TARGET_FOLDER, split_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trained images: 1126\n",
      "(360, 480, 3)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot image\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pathname = 'train/masks/img/'\n",
    "\n",
    "list_files = os.listdir(TARGET_FOLDER + pathname)\n",
    "nb_files = len(list_files)\n",
    "print('Number of trained images:', nb_files)\n",
    " \n",
    "img_example = cv2.imread(TARGET_FOLDER + pathname + list_files[1])\n",
    "# cv2.imshow('Example', img_example) \n",
    "# cv2.waitKey(0)\n",
    "\n",
    "print(img_example.shape)\n",
    "\n",
    "plt.hist(img_example[:,:,0].ravel(), 256, [0,10]);\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_reshape(img):\n",
    "    return img[:,:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1126 images belonging to 1 classes.\n",
      "Found 1126 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "# Data augmentation (for train dataset)\n",
    "SEED = 1\n",
    "\n",
    "train_image_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale = 1/255.,\n",
    "                                                                horizontal_flip=True,\n",
    "                                                                rotation_range=20,\n",
    "                                                                zoom_range=0.2,\n",
    "                                                                width_shift_range = 0.1,\n",
    "                                                                height_shift_range = 0.1)\n",
    "train_image_generator= train_image_datagen.flow_from_directory(TARGET_FOLDER + 'train/images',\n",
    "                                                               batch_size=10,\n",
    "                                                               target_size=TARGET_SIZE,\n",
    "                                                               class_mode=None,\n",
    "                                                               seed=SEED)\n",
    "\n",
    "train_mask_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "                                                                horizontal_flip=True,\n",
    "                                                                rotation_range=20,\n",
    "                                                                zoom_range=0.2,\n",
    "                                                                width_shift_range = 0.1,\n",
    "                                                                height_shift_range = 0.1,\n",
    "                                                                preprocessing_function=preprocessing_reshape)\n",
    "train_mask_generator= train_image_datagen.flow_from_directory(TARGET_FOLDER + 'train/masks/',\n",
    "                                                               batch_size=10,\n",
    "                                                               target_size=TARGET_SIZE,\n",
    "                                                               class_mode=None,\n",
    "                                                               seed=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 126 images belonging to 1 classes.\n",
      "Found 126 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "# Data augmentation (for validation dataset)\n",
    "SEED = 1\n",
    "\n",
    "val_image_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale = 1/255.)\n",
    "val_image_generator= val_image_datagen.flow_from_directory(TARGET_FOLDER + 'test/images',\n",
    "                                                               batch_size=10,\n",
    "                                                               target_size=TARGET_SIZE,\n",
    "                                                               class_mode=None,\n",
    "                                                               seed=SEED)\n",
    "\n",
    "val_mask_datagen = tf.keras.preprocessing.image.ImageDataGenerator(preprocessing_function=preprocessing_reshape)\n",
    "val_mask_generator= val_image_datagen.flow_from_directory(TARGET_FOLDER + 'test/masks/',\n",
    "                                                               batch_size=10,\n",
    "                                                               target_size=TARGET_SIZE,\n",
    "                                                               class_mode=None,\n",
    "                                                               seed=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create generator\n",
    "def my_image_mask_generator(image_data_generator, mask_data_generator):\n",
    "    new_generator = zip(image_data_generator, mask_data_generator)\n",
    "    for (img, mask) in new_generator:\n",
    "        yield (img, mask, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameter describing where the channel dimension is found in our dataset\n",
    "IMAGE_ORDERING = 'channels_last'\n",
    "\n",
    "def conv_block(input, filters, strides, pooling_size, pool_strides):\n",
    "  '''\n",
    "  Args:\n",
    "    input (tensor) -- batch of images or features\n",
    "    filters (int) -- number of filters of the Conv2D layers\n",
    "    strides (int) -- strides setting of the Conv2D layers\n",
    "    pooling_size (int) -- pooling size of the MaxPooling2D layers\n",
    "    pool_strides (int) -- strides setting of the MaxPooling2D layers\n",
    "  \n",
    "  Returns:\n",
    "    (tensor) max pooled and batch-normalized features of the input \n",
    "  '''\n",
    "  # use the functional syntax to stack the layers as shown in the diagram above\n",
    "  x = tf.keras.layers.Conv2D(filters, strides, padding='same', data_format=IMAGE_ORDERING)(input)\n",
    "  x= tf.keras.layers.LeakyReLU()(x)\n",
    "  x = tf.keras.layers.Conv2D(filters, strides, padding='same')(x)\n",
    "  x= tf.keras.layers.LeakyReLU()(x)\n",
    "  x = tf.keras.layers.MaxPooling2D(pool_size=pooling_size, strides=pool_strides)(x)\n",
    "  x = tf.keras.layers.BatchNormalization()(x)\n",
    "\n",
    "  return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_13\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_19 (InputLayer)        [(None, 64, 64, 3)]       0         \n",
      "_________________________________________________________________\n",
      "conv2d_162 (Conv2D)          (None, 64, 64, 32)        896       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_140 (LeakyReLU)  (None, 64, 64, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_163 (Conv2D)          (None, 64, 64, 32)        9248      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_141 (LeakyReLU)  (None, 64, 64, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_70 (MaxPooling (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_70 (Batc (None, 32, 32, 32)        128       \n",
      "=================================================================\n",
      "Total params: 10,272\n",
      "Trainable params: 10,208\n",
      "Non-trainable params: 64\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# TEST CODE:\n",
    "\n",
    "test_input = tf.keras.layers.Input(shape=(HEIGHT, WIDTH, 3))\n",
    "test_output = conv_block(test_input, 32, 3, 2, 2)\n",
    "test_model = tf.keras.Model(inputs=test_input, outputs=test_output)\n",
    "\n",
    "print(test_model.summary())\n",
    "\n",
    "# free up test resources\n",
    "del test_input, test_output, test_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FCN8(input_height=HEIGHT, input_width=WIDTH):\n",
    "    '''\n",
    "    Defines the downsampling path of the image segmentation model.\n",
    "\n",
    "    Args:\n",
    "      input_height (int) -- height of the images\n",
    "      width (int) -- width of the images\n",
    "\n",
    "    Returns:\n",
    "    (tuple of tensors, tensor)\n",
    "      tuple of tensors -- features extracted at blocks 3 to 5\n",
    "      tensor -- copy of the input\n",
    "    '''\n",
    "   \n",
    "    img_input = tf.keras.layers.Input(shape=(input_height,input_width, 3))\n",
    "    \n",
    "    # pad the input image to have dimensions to the nearest power of two\n",
    "    x = tf.keras.layers.ZeroPadding2D(padding=(0, 0))(img_input)\n",
    "\n",
    "    # Block 1\n",
    "    x = conv_block(x, filters=32, strides=(3, 3), pooling_size=(2, 2), pool_strides=(2, 2))\n",
    "    \n",
    "    # Block 2\n",
    "    x = conv_block(x, filters=64, strides=(3, 3), pooling_size=(2, 2), pool_strides=(2, 2))\n",
    "\n",
    "    # Block 3\n",
    "    x = conv_block(x, filters=128, strides=(3, 3), pooling_size=(2, 2), pool_strides=(2, 2))\n",
    "    # save the feature map at this stage\n",
    "    f3 = x\n",
    "\n",
    "    # Block 4\n",
    "    x = conv_block(x, filters=256, strides=(3, 3), pooling_size=(2, 2), pool_strides=(2, 2))\n",
    "    # save the feature map at this stage\n",
    "    f4 = x\n",
    "\n",
    "    # Block 5\n",
    "    x = conv_block(x, filters=256, strides=(3, 3), pooling_size=(2, 2), pool_strides=(2, 2))\n",
    "    # save the feature map at this stage\n",
    "    f5 = x\n",
    "  \n",
    "    return (f3, f4, f5), img_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_14\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_20 (InputLayer)        [(None, 64, 64, 3)]       0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_13 (ZeroPaddi (None, 64, 64, 3)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_164 (Conv2D)          (None, 64, 64, 32)        896       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_142 (LeakyReLU)  (None, 64, 64, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_165 (Conv2D)          (None, 64, 64, 32)        9248      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_143 (LeakyReLU)  (None, 64, 64, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_71 (MaxPooling (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_71 (Batc (None, 32, 32, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_166 (Conv2D)          (None, 32, 32, 64)        18496     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_144 (LeakyReLU)  (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_167 (Conv2D)          (None, 32, 32, 64)        36928     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_145 (LeakyReLU)  (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_72 (MaxPooling (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_72 (Batc (None, 16, 16, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_168 (Conv2D)          (None, 16, 16, 128)       73856     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_146 (LeakyReLU)  (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_169 (Conv2D)          (None, 16, 16, 128)       147584    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_147 (LeakyReLU)  (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_73 (MaxPooling (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_73 (Batc (None, 8, 8, 128)         512       \n",
      "_________________________________________________________________\n",
      "conv2d_170 (Conv2D)          (None, 8, 8, 256)         295168    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_148 (LeakyReLU)  (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_171 (Conv2D)          (None, 8, 8, 256)         590080    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_149 (LeakyReLU)  (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_74 (MaxPooling (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_74 (Batc (None, 4, 4, 256)         1024      \n",
      "_________________________________________________________________\n",
      "conv2d_172 (Conv2D)          (None, 4, 4, 256)         590080    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_150 (LeakyReLU)  (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_173 (Conv2D)          (None, 4, 4, 256)         590080    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_151 (LeakyReLU)  (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_75 (MaxPooling (None, 2, 2, 256)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_75 (Batc (None, 2, 2, 256)         1024      \n",
      "=================================================================\n",
      "Total params: 2,355,360\n",
      "Trainable params: 2,353,888\n",
      "Non-trainable params: 1,472\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# TEST CODE:\n",
    "\n",
    "test_convs, test_img_input = FCN8()\n",
    "test_model = tf.keras.Model(inputs=test_img_input, outputs=[test_convs, test_img_input])\n",
    "\n",
    "print(test_model.summary())\n",
    "\n",
    "del test_convs, test_img_input, test_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fcn8_decoder(convs, n_classes):\n",
    "  # features from the encoder stage\n",
    "  f3, f4, f5 = convs\n",
    "\n",
    "  # number of filters\n",
    "  n = 512\n",
    "\n",
    "  # add convolutional layers on top of the CNN extractor.\n",
    "  o = tf.keras.layers.Conv2D(n , (7 , 7) , activation='relu' , padding='same', name=\"conv6\", data_format=IMAGE_ORDERING)(f5)\n",
    "  o = tf.keras.layers.Dropout(0.5)(o)\n",
    "\n",
    "  o = tf.keras.layers.Conv2D(n , (1 , 1) , activation='relu' , padding='same', name=\"conv7\", data_format=IMAGE_ORDERING)(o)\n",
    "  o = tf.keras.layers.Dropout(0.5)(o)\n",
    "\n",
    "  o = tf.keras.layers.Conv2D(n_classes,  (1, 1), activation='relu' , padding='same', data_format=IMAGE_ORDERING)(o)\n",
    "\n",
    "\n",
    "  # Upsample `o` above and crop any extra pixels introduced\n",
    "  o = tf.keras.layers.Conv2DTranspose(n_classes , kernel_size=(4,4) ,  strides=(2,2) , use_bias=False )(f5)\n",
    "  o = tf.keras.layers.Cropping2D(cropping=(1,1))(o)\n",
    "\n",
    "  # load the pool 4 prediction and do a 1x1 convolution to reshape it to the same shape of `o` above\n",
    "  o2 = f4\n",
    "  o2 = tf.keras.layers.Conv2D(n_classes, kernel_size=(1, 1), activation='relu', padding='same', data_format=IMAGE_ORDERING)(o2)\n",
    "\n",
    "  # add the results of the upsampling and pool 4 prediction\n",
    "  o = tf.keras.layers.Add()([o, o2])\n",
    "\n",
    "  # upsample the resulting tensor of the operation you just did\n",
    "  o = tf.keras.layers.Conv2DTranspose(n_classes , kernel_size=(4,4) ,  strides=(2,2) , use_bias=False )(o)\n",
    "  o = tf.keras.layers.Cropping2D(cropping=(1,1))(o)\n",
    "\n",
    "  # load the pool 3 prediction and do a 1x1 convolution to reshape it to the same shape of `o` above\n",
    "  o2 = f3\n",
    "  o2 = tf.keras.layers.Conv2D(n_classes , ( 1 , 1 ) , activation='relu' , padding='same', data_format=IMAGE_ORDERING)(o2)\n",
    "\n",
    "  # add the results of the upsampling and pool 3 prediction\n",
    "  o = tf.keras.layers.Add()([o, o2])\n",
    "\n",
    "  # upsample up to the size of the original image\n",
    "  o = tf.keras.layers.Conv2DTranspose(n_classes , kernel_size=(8,8) ,  strides=(8,8) , use_bias=False )(o)\n",
    "  o = tf.keras.layers.Cropping2D(((0, HEIGHT-HEIGHT), (0, 0)))(o)\n",
    "\n",
    "  # append a sigmoid activation\n",
    "  o = (tf.keras.layers.Activation('sigmoid'))(o)\n",
    "\n",
    "  return o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 64, 64, 1)\n"
     ]
    }
   ],
   "source": [
    "# TEST CODE\n",
    "\n",
    "test_convs, test_img_input = FCN8()\n",
    "test_fcn8_decoder = fcn8_decoder(test_convs, 1)\n",
    "\n",
    "print(test_fcn8_decoder.shape)\n",
    "\n",
    "del test_convs, test_img_input, test_fcn8_decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 64, 64, 3)\n"
     ]
    }
   ],
   "source": [
    "# start the encoder using the default input size 64 x 84\n",
    "convs, img_input = FCN8()\n",
    "\n",
    "# pass the convolutions obtained in the encoder to the decoder\n",
    "n_classes = 3\n",
    "dec_op = fcn8_decoder(convs, n_classes)\n",
    "\n",
    "print(dec_op.shape)\n",
    "\n",
    "# define the model specifying the input (batch of images) and output (decoder output)\n",
    "model = tf.keras.Model(inputs = img_input, outputs = dec_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "   5320/Unknown - 1486s 279ms/step - loss: 0.0443 - accuracy: 0.9270"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-109-86374a219ddb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m model.fit(my_train_generator,\n\u001b[0;32m     12\u001b[0m                    \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m                    verbose=1)\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;31m#                    validation_data=my_val_generator)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 819\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    820\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    340\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    341\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 342\u001b[1;33m                 total_epochs=epochs)\n\u001b[0m\u001b[0;32m    343\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    344\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[1;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[0;32m    126\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[0;32m    127\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 128\u001b[1;33m         \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    129\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m         \u001b[1;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[1;34m(input_fn)\u001b[0m\n\u001b[0;32m     96\u001b[0m     \u001b[1;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[1;32m---> 98\u001b[1;33m                               distributed_function(input_fn))\n\u001b[0m\u001b[0;32m     99\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    566\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    567\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 568\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    569\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    570\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    597\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    598\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 599\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    600\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    601\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2361\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2362\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2363\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2364\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2365\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   1609\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[0;32m   1610\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[1;32m-> 1611\u001b[1;33m         self.captured_inputs)\n\u001b[0m\u001b[0;32m   1612\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1613\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1690\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1692\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1694\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    543\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"executor_type\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"config_proto\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    547\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[0;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m                                                num_outputs)\n\u001b[0m\u001b[0;32m     62\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Create custom generator for training images and masks\n",
    "my_train_generator = my_image_mask_generator(train_image_generator, train_mask_generator)\n",
    "my_val_generator = my_image_mask_generator(val_image_generator, val_mask_generator)\n",
    "\n",
    "# Compile your model here\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(lr = 1e-4),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train your model here\n",
    "model.fit(my_train_generator,\n",
    "                   epochs=2,\n",
    "                   verbose=1)\n",
    "#                    validation_data=my_val_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
