{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import random\n",
    "from shutil import copyfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "HEIGHT = 128\n",
    "WIDTH = 128\n",
    "BATCH_SIZE = 10\n",
    "SPLIT_RATIO = 0.8\n",
    "TARGET_SIZE = (HEIGHT, WIDTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unzip file\n",
    "import tarfile\n",
    "\n",
    "filename = 'dataset/synthetic_sugarbeet_random_weeds'\n",
    "\n",
    "# my_tar = tarfile.open(filename + '.tar.gz')\n",
    "# my_tar.extractall('dataset')\n",
    "# my_tar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "def make_directory(fullpath):\n",
    "    splitted_data = fullpath.split('/')\n",
    "    \n",
    "    new_dir = []\n",
    "    current_dir = '.'\n",
    "    \n",
    "    for folder in splitted_data:           \n",
    "        current_dir = current_dir + '/' + folder                \n",
    "        if not os.path.isdir(current_dir + '/'):\n",
    "            new_dir.append(current_dir)\n",
    "            os.mkdir(current_dir)\n",
    "    \n",
    "    return new_dir\n",
    "    \n",
    "print(make_directory('dataset/synthetic_sugarbeet_random_weeds/tom/fougere/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a python function called split_data which takes\n",
    "# a FOLDER_IMAGES directory containing the images\n",
    "# a FOLDER_MASKS directory containing the masks\n",
    "# a TARGET_FOLDER directory where the files will be copied to\n",
    "# the TARGET_FOLDER directory will contain 2 subfolders train and test with the data splitted\n",
    "# a SPLIT SIZE to determine the portion\n",
    "# The files should also be randomized, so that the training set is a random\n",
    "# X% of the files, and the test set is the remaining files\n",
    "# SO, for example, if SPLIT SIZE is .9\n",
    "# Then 90% of the images will be copied to the TRAINING dir\n",
    "# and 10% of the images will be copied to the TESTING dir\n",
    "# Also -- All images should be checked, and if they have a zero file length,\n",
    "# they will not be copied over\n",
    "def split_data(FOLDER_IMAGES, FOLDER_MASKS, TARGET_FOLDER, SPLIT_SIZE):\n",
    "\n",
    "    TRAINING_FOLDER_NAME = TARGET_FOLDER + '/train/'\n",
    "    TESTING_FOLDER_NAME = TARGET_FOLDER + '/test/'\n",
    "    TRAINING_FOLDER_NAME_IMAGES = TARGET_FOLDER + '/train/images/img/'\n",
    "    TRAINING_FOLDER_NAME_MASKS = TARGET_FOLDER + '/train/masks/img/'\n",
    "    TESTING_FOLDER_NAME_IMAGES = TARGET_FOLDER + '/test/images/img/'\n",
    "    TESTING_FOLDER_NAME_MASKS = TARGET_FOLDER + '/test/masks/img/'\n",
    "\n",
    "    # Create directories\n",
    "    make_directory(TRAINING_FOLDER_NAME_IMAGES)\n",
    "    make_directory(TRAINING_FOLDER_NAME_MASKS)\n",
    "    make_directory(TESTING_FOLDER_NAME_IMAGES)\n",
    "    make_directory(TESTING_FOLDER_NAME_MASKS)\n",
    "    \n",
    "    # Remove all data in TRAINING and TESTING dir\n",
    "    for i_file in os.listdir(TRAINING_FOLDER_NAME_IMAGES):\n",
    "        os.remove(TRAINING_FOLDER_NAME_IMAGES + i_file)\n",
    "    for i_file in os.listdir(TRAINING_FOLDER_NAME_MASKS):\n",
    "        os.remove(TRAINING_FOLDER_NAME_MASKS + i_file)\n",
    "    for i_file in os.listdir(TESTING_FOLDER_NAME_IMAGES):\n",
    "        os.remove(TESTING_FOLDER_NAME_IMAGES + i_file)\n",
    "    for i_file in os.listdir(TESTING_FOLDER_NAME_MASKS):\n",
    "        os.remove(TESTING_FOLDER_NAME_MASKS + i_file)\n",
    "    \n",
    "    dataset = []\n",
    "    \n",
    "    # Check for z zero file length\n",
    "    for i_file in os.listdir(FOLDER_IMAGES):\n",
    "        data = i_file\n",
    "        if (os.path.getsize(FOLDER_IMAGES + data) > 0):\n",
    "            dataset.append(i_file)\n",
    "        else:\n",
    "            print('Skipped ' + i_file)\n",
    "            print('Invalid file size! i.e Zero length.')\n",
    "    \n",
    "    # Number of files\n",
    "    nb_files = len(dataset)\n",
    "    nb_files_training = int(nb_files * SPLIT_SIZE)\n",
    "    nb_files_testing = nb_files - nb_files_training\n",
    "    \n",
    "    # Suffle dataset\n",
    "    shuffled_dataset = random.sample(dataset, len(dataset))\n",
    "        \n",
    "    # Copy files\n",
    "    for i_num, i_file in enumerate(shuffled_dataset):\n",
    "        if i_num < nb_files_training:\n",
    "            new_path_images = TRAINING_FOLDER_NAME_IMAGES + i_file\n",
    "            new_path_masks = TRAINING_FOLDER_NAME_MASKS + i_file\n",
    "        else:\n",
    "            new_path_images = TESTING_FOLDER_NAME_IMAGES + i_file\n",
    "            new_path_masks = TESTING_FOLDER_NAME_MASKS + i_file\n",
    "            \n",
    "        copyfile(FOLDER_IMAGES + i_file, new_path_images)\n",
    "        copyfile(FOLDER_MASKS + i_file, new_path_masks)\n",
    "        \n",
    "    return nb_files_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "FOLDER_IMAGES = \"dataset/synthetic_sugarbeet_random_weeds/rgb/\"\n",
    "FOLDER_MASKS = \"dataset/synthetic_sugarbeet_random_weeds/gt/\"\n",
    "TARGET_FOLDER = \"dataset/synthetic_sugarbeet_random_weeds/train_test/\"\n",
    "\n",
    "NB_TRAINED_IMAGES = split_data(FOLDER_IMAGES, FOLDER_MASKS, TARGET_FOLDER, SPLIT_RATIO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trained images: 1001\n",
      "(360, 480, 3)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot image\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pathname = 'train/masks/img/'\n",
    "\n",
    "list_files = os.listdir(TARGET_FOLDER + pathname)\n",
    "nb_files = len(list_files)\n",
    "print('Number of trained images:', nb_files)\n",
    " \n",
    "img_example = cv2.imread(TARGET_FOLDER + pathname + list_files[1])\n",
    "# cv2.imshow('Example', img_example) \n",
    "# cv2.waitKey(0)\n",
    "\n",
    "print(img_example.shape)\n",
    "\n",
    "plt.hist(img_example[:,:,0].ravel(), 256, [0,10]);\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_reshape(img):\n",
    "    return img[:,:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1001 images belonging to 1 classes.\n",
      "Found 1001 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "# Data augmentation (for train dataset)\n",
    "SEED = 1\n",
    "\n",
    "train_image_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale = 1/255.,\n",
    "                                                                horizontal_flip=True,\n",
    "                                                                rotation_range=20,\n",
    "                                                                zoom_range=0.2,\n",
    "                                                                width_shift_range = 0.1,\n",
    "                                                                height_shift_range = 0.1)\n",
    "train_image_generator= train_image_datagen.flow_from_directory(TARGET_FOLDER + 'train/images',\n",
    "                                                               batch_size=BATCH_SIZE,\n",
    "                                                               target_size=TARGET_SIZE,\n",
    "                                                               class_mode=None,\n",
    "                                                               seed=SEED)\n",
    "\n",
    "train_mask_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "                                                                horizontal_flip=True,\n",
    "                                                                rotation_range=20,\n",
    "                                                                zoom_range=0.2,\n",
    "                                                                width_shift_range = 0.1,\n",
    "                                                                height_shift_range = 0.1,\n",
    "                                                                preprocessing_function=preprocessing_reshape)\n",
    "train_mask_generator= train_image_datagen.flow_from_directory(TARGET_FOLDER + 'train/masks/',\n",
    "                                                               batch_size=BATCH_SIZE,\n",
    "                                                               target_size=TARGET_SIZE,\n",
    "                                                               class_mode=None,\n",
    "                                                               seed=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 251 images belonging to 1 classes.\n",
      "Found 251 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "# Data augmentation (for validation dataset)\n",
    "SEED = 1\n",
    "\n",
    "val_image_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale = 1/255.)\n",
    "val_image_generator= val_image_datagen.flow_from_directory(TARGET_FOLDER + 'test/images',\n",
    "                                                               batch_size=BATCH_SIZE,\n",
    "                                                               target_size=TARGET_SIZE,\n",
    "                                                               class_mode=None,\n",
    "                                                               seed=SEED)\n",
    "\n",
    "val_mask_datagen = tf.keras.preprocessing.image.ImageDataGenerator(preprocessing_function=preprocessing_reshape)\n",
    "val_mask_generator= val_image_datagen.flow_from_directory(TARGET_FOLDER + 'test/masks/',\n",
    "                                                               batch_size=BATCH_SIZE,\n",
    "                                                               target_size=TARGET_SIZE,\n",
    "                                                               class_mode=None,\n",
    "                                                               seed=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create generator\n",
    "def my_image_mask_generator(image_data_generator, mask_data_generator):\n",
    "    new_generator = zip(image_data_generator, mask_data_generator)\n",
    "    for (img, mask) in new_generator:\n",
    "        yield (img, mask, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameter describing where the channel dimension is found in our dataset\n",
    "IMAGE_ORDERING = 'channels_last'\n",
    "\n",
    "def conv_block(input, filters, strides, pooling_size, pool_strides):\n",
    "  '''\n",
    "  Args:\n",
    "    input (tensor) -- batch of images or features\n",
    "    filters (int) -- number of filters of the Conv2D layers\n",
    "    strides (int) -- strides setting of the Conv2D layers\n",
    "    pooling_size (int) -- pooling size of the MaxPooling2D layers\n",
    "    pool_strides (int) -- strides setting of the MaxPooling2D layers\n",
    "  \n",
    "  Returns:\n",
    "    (tensor) max pooled and batch-normalized features of the input \n",
    "  '''\n",
    "  # use the functional syntax to stack the layers as shown in the diagram above\n",
    "  x = tf.keras.layers.Conv2D(filters, strides, padding='same', data_format=IMAGE_ORDERING)(input)\n",
    "  x= tf.keras.layers.LeakyReLU()(x)\n",
    "  x = tf.keras.layers.Conv2D(filters, strides, padding='same')(x)\n",
    "  x = tf.keras.layers.LeakyReLU()(x)\n",
    "  x = tf.keras.layers.MaxPooling2D(pool_size=pooling_size, strides=pool_strides)(x)\n",
    "  x = tf.keras.layers.BatchNormalization()(x)\n",
    "\n",
    "  return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_6 (InputLayer)         [(None, 128, 128, 3)]     0         \n",
      "_________________________________________________________________\n",
      "conv2d_51 (Conv2D)           (None, 128, 128, 32)      896       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_42 (LeakyReLU)   (None, 128, 128, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_52 (Conv2D)           (None, 128, 128, 32)      9248      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_43 (LeakyReLU)   (None, 128, 128, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_21 (MaxPooling (None, 64, 64, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_21 (Batc (None, 64, 64, 32)        128       \n",
      "=================================================================\n",
      "Total params: 10,272\n",
      "Trainable params: 10,208\n",
      "Non-trainable params: 64\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# TEST CODE:\n",
    "\n",
    "test_input = tf.keras.layers.Input(shape=(HEIGHT, WIDTH, 3))\n",
    "test_output = conv_block(test_input, 32, 3, 2, 2)\n",
    "test_model = tf.keras.Model(inputs=test_input, outputs=test_output)\n",
    "\n",
    "print(test_model.summary())\n",
    "\n",
    "# free up test resources\n",
    "del test_input, test_output, test_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FCN8(input_height=HEIGHT, input_width=WIDTH):\n",
    "    '''\n",
    "    Defines the downsampling path of the image segmentation model.\n",
    "\n",
    "    Args:\n",
    "      input_height (int) -- height of the images\n",
    "      width (int) -- width of the images\n",
    "\n",
    "    Returns:\n",
    "    (tuple of tensors, tensor)\n",
    "      tuple of tensors -- features extracted at blocks 3 to 5\n",
    "      tensor -- copy of the input\n",
    "    '''\n",
    "   \n",
    "    img_input = tf.keras.layers.Input(shape=(input_height,input_width, 3))\n",
    "    \n",
    "    # pad the input image to have dimensions to the nearest power of two\n",
    "    x = tf.keras.layers.ZeroPadding2D(padding=(0, 0))(img_input)\n",
    "\n",
    "    # Block 1\n",
    "    x = conv_block(x, filters=32, strides=(3, 3), pooling_size=(2, 2), pool_strides=(2, 2))\n",
    "    \n",
    "    # Block 2\n",
    "    x = conv_block(x, filters=64, strides=(3, 3), pooling_size=(2, 2), pool_strides=(2, 2))\n",
    "\n",
    "    # Block 3\n",
    "    x = conv_block(x, filters=128, strides=(3, 3), pooling_size=(2, 2), pool_strides=(2, 2))\n",
    "    # save the feature map at this stage\n",
    "    f3 = x\n",
    "\n",
    "    # Block 4\n",
    "    x = conv_block(x, filters=256, strides=(3, 3), pooling_size=(2, 2), pool_strides=(2, 2))\n",
    "    # save the feature map at this stage\n",
    "    f4 = x\n",
    "\n",
    "    # Block 5\n",
    "    x = conv_block(x, filters=256, strides=(3, 3), pooling_size=(2, 2), pool_strides=(2, 2))\n",
    "    # save the feature map at this stage\n",
    "    f5 = x\n",
    "  \n",
    "    return (f3, f4, f5), img_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_7 (InputLayer)         [(None, 128, 128, 3)]     0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_4 (ZeroPaddin (None, 128, 128, 3)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_53 (Conv2D)           (None, 128, 128, 32)      896       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_44 (LeakyReLU)   (None, 128, 128, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_54 (Conv2D)           (None, 128, 128, 32)      9248      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_45 (LeakyReLU)   (None, 128, 128, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_22 (MaxPooling (None, 64, 64, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_22 (Batc (None, 64, 64, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_55 (Conv2D)           (None, 64, 64, 64)        18496     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_46 (LeakyReLU)   (None, 64, 64, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_56 (Conv2D)           (None, 64, 64, 64)        36928     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_47 (LeakyReLU)   (None, 64, 64, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_23 (MaxPooling (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_23 (Batc (None, 32, 32, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_57 (Conv2D)           (None, 32, 32, 128)       73856     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_48 (LeakyReLU)   (None, 32, 32, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_58 (Conv2D)           (None, 32, 32, 128)       147584    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_49 (LeakyReLU)   (None, 32, 32, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_24 (MaxPooling (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_24 (Batc (None, 16, 16, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv2d_59 (Conv2D)           (None, 16, 16, 256)       295168    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_50 (LeakyReLU)   (None, 16, 16, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_60 (Conv2D)           (None, 16, 16, 256)       590080    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_51 (LeakyReLU)   (None, 16, 16, 256)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_25 (MaxPooling (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_25 (Batc (None, 8, 8, 256)         1024      \n",
      "_________________________________________________________________\n",
      "conv2d_61 (Conv2D)           (None, 8, 8, 256)         590080    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_52 (LeakyReLU)   (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_62 (Conv2D)           (None, 8, 8, 256)         590080    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_53 (LeakyReLU)   (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_26 (MaxPooling (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_26 (Batc (None, 4, 4, 256)         1024      \n",
      "=================================================================\n",
      "Total params: 2,355,360\n",
      "Trainable params: 2,353,888\n",
      "Non-trainable params: 1,472\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# TEST CODE:\n",
    "test_convs, test_img_input = FCN8()\n",
    "test_model = tf.keras.Model(inputs=test_img_input, outputs=[test_convs, test_img_input])\n",
    "\n",
    "print(test_model.summary())\n",
    "\n",
    "del test_convs, test_img_input, test_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fcn8_decoder(convs, n_classes):\n",
    "  # features from the encoder stage\n",
    "  f3, f4, f5 = convs\n",
    "\n",
    "  # number of filters\n",
    "  n = 512\n",
    "\n",
    "  # add convolutional layers on top of the CNN extractor.\n",
    "  o = tf.keras.layers.Conv2D(n , (7 , 7) , activation='relu' , padding='same', name=\"conv6\", data_format=IMAGE_ORDERING)(f5)\n",
    "  o = tf.keras.layers.Dropout(0.5)(o)\n",
    "\n",
    "  o = tf.keras.layers.Conv2D(n , (1 , 1) , activation='relu' , padding='same', name=\"conv7\", data_format=IMAGE_ORDERING)(o)\n",
    "  o = tf.keras.layers.Dropout(0.5)(o)\n",
    "\n",
    "  o = tf.keras.layers.Conv2D(n_classes,  (1, 1), activation='relu' , padding='same', data_format=IMAGE_ORDERING)(o)\n",
    "\n",
    "\n",
    "  # Upsample `o` above and crop any extra pixels introduced\n",
    "  o = tf.keras.layers.Conv2DTranspose(n_classes , kernel_size=(4,4) ,  strides=(2,2) , use_bias=False )(f5)\n",
    "  o = tf.keras.layers.Cropping2D(cropping=(1,1))(o)\n",
    "\n",
    "  # load the pool 4 prediction and do a 1x1 convolution to reshape it to the same shape of `o` above\n",
    "  o2 = f4\n",
    "  o2 = tf.keras.layers.Conv2D(n_classes, kernel_size=(1, 1), activation='relu', padding='same', data_format=IMAGE_ORDERING)(o2)\n",
    "\n",
    "  # add the results of the upsampling and pool 4 prediction\n",
    "  o = tf.keras.layers.Add()([o, o2])\n",
    "\n",
    "  # upsample the resulting tensor of the operation you just did\n",
    "  o = tf.keras.layers.Conv2DTranspose(n_classes , kernel_size=(4,4) ,  strides=(2,2) , use_bias=False )(o)\n",
    "  o = tf.keras.layers.Cropping2D(cropping=(1,1))(o)\n",
    "\n",
    "  # load the pool 3 prediction and do a 1x1 convolution to reshape it to the same shape of `o` above\n",
    "  o2 = f3\n",
    "  o2 = tf.keras.layers.Conv2D(n_classes , ( 1 , 1 ) , activation='relu' , padding='same', data_format=IMAGE_ORDERING)(o2)\n",
    "\n",
    "  # add the results of the upsampling and pool 3 prediction\n",
    "  o = tf.keras.layers.Add()([o, o2])\n",
    "\n",
    "  # upsample up to the size of the original image\n",
    "  o = tf.keras.layers.Conv2DTranspose(n_classes , kernel_size=(8,8) ,  strides=(8,8) , use_bias=False )(o)\n",
    "  o = tf.keras.layers.Cropping2D(((0, HEIGHT-HEIGHT), (0, 0)))(o)\n",
    "\n",
    "  # append a sigmoid activation\n",
    "  o = (tf.keras.layers.Activation('sigmoid'))(o)\n",
    "\n",
    "  return o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 128, 128, 1)\n"
     ]
    }
   ],
   "source": [
    "# TEST CODE:\n",
    "test_convs, test_img_input = FCN8()\n",
    "test_fcn8_decoder = fcn8_decoder(test_convs, 1)\n",
    "\n",
    "print(test_fcn8_decoder.shape)\n",
    "\n",
    "del test_convs, test_img_input, test_fcn8_decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 128, 128, 1)\n"
     ]
    }
   ],
   "source": [
    "# start the encoder using the default input size 64 x 84\n",
    "convs, img_input = FCN8()\n",
    "\n",
    "# pass the convolutions obtained in the encoder to the decoder\n",
    "n_classes = 1\n",
    "dec_op = fcn8_decoder(convs, n_classes)\n",
    "\n",
    "print(dec_op.shape)\n",
    "\n",
    "# define the model specifying the input (batch of images) and output (decoder output)\n",
    "model = tf.keras.Model(inputs = img_input, outputs = dec_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 100.1 steps, validate for 100.1 steps\n",
      "Epoch 1/10\n",
      "101/100 [==============================] - 120s 1s/step - loss: 0.6907 - accuracy: 0.5204 - val_loss: 0.6966 - val_accuracy: 0.5548\n",
      "Epoch 2/10\n",
      "101/100 [==============================] - 104s 1s/step - loss: 0.6749 - accuracy: 0.5659 - val_loss: 0.6743 - val_accuracy: 0.5995\n",
      "Epoch 3/10\n",
      "101/100 [==============================] - 108s 1s/step - loss: 0.6423 - accuracy: 0.6223 - val_loss: 0.6353 - val_accuracy: 0.6450\n",
      "Epoch 4/10\n",
      "101/100 [==============================] - 138s 1s/step - loss: 0.5992 - accuracy: 0.6690 - val_loss: 0.5862 - val_accuracy: 0.6904\n",
      "Epoch 5/10\n",
      "101/100 [==============================] - 106s 1s/step - loss: 0.5496 - accuracy: 0.6876 - val_loss: 0.5322 - val_accuracy: 0.7053\n",
      "Epoch 6/10\n",
      "101/100 [==============================] - 110s 1s/step - loss: 0.4966 - accuracy: 0.7004 - val_loss: 0.4758 - val_accuracy: 0.7206\n",
      "Epoch 7/10\n",
      "101/100 [==============================] - 108s 1s/step - loss: 0.4424 - accuracy: 0.7530 - val_loss: 0.4204 - val_accuracy: 0.7807\n",
      "Epoch 8/10\n",
      "101/100 [==============================] - 92s 906ms/step - loss: 0.3881 - accuracy: 0.7802 - val_loss: 0.3644 - val_accuracy: 0.7955\n",
      "Epoch 9/10\n",
      "101/100 [==============================] - 97s 956ms/step - loss: 0.3340 - accuracy: 0.7891 - val_loss: 0.3105 - val_accuracy: 0.7956\n",
      "Epoch 10/10\n",
      "101/100 [==============================] - 101s 1000ms/step - loss: 0.2803 - accuracy: 0.7886 - val_loss: 0.2558 - val_accuracy: 0.7957\n"
     ]
    }
   ],
   "source": [
    "# Create custom generator for training images and masks\n",
    "my_train_generator = my_image_mask_generator(train_image_generator, train_mask_generator)\n",
    "my_val_generator = my_image_mask_generator(val_image_generator, val_mask_generator)\n",
    "\n",
    "# Compile your model here\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(lr = 1e-4),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "steps = NB_TRAINED_IMAGES / BATCH_SIZE\n",
    "\n",
    "# Train your model here\n",
    "history = model.fit(my_train_generator,\n",
    "                    steps_per_epoch=steps,\n",
    "                    epochs=10,\n",
    "                    verbose=1,\n",
    "                    validation_data=my_val_generator,\n",
    "                    validation_steps=steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image):\n",
    "    img = tf.cast(image, tf.float32)\n",
    "    img = tf.image.resize(img, TARGET_SIZE)\n",
    "    img = img / 255.\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Plot image\n",
    "example_id = 2\n",
    "pathname = 'test/images/img/'\n",
    "\n",
    "list_files = os.listdir(TARGET_FOLDER + pathname)\n",
    " \n",
    "img_example = cv2.imread(TARGET_FOLDER + pathname + list_files[example_id])\n",
    "mask_example = cv2.imread(TARGET_FOLDER + 'test/masks/img/' + list_files[example_id])\n",
    "\n",
    "image_processed = preprocess_image(img_example)\n",
    "image_processed = tf.expand_dims(image_processed, axis=0)\n",
    "\n",
    "prediction = model.predict(image_processed)\n",
    "prediction = tf.cast(prediction > 0.5, tf.uint8)\n",
    "pred_image = np.array(tf.squeeze(prediction, axis=0)*255)\n",
    "\n",
    "\n",
    "image_example_gt = np.concatenate((img_example, mask_example*255), axis=1)\n",
    "cv2.imshow('Example', pred_image) \n",
    "cv2.waitKey(0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
