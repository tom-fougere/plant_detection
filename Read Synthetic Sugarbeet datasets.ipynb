{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import random\n",
    "from shutil import copyfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "HEIGHT = 64\n",
    "WIDTH = 64\n",
    "BATCH_SIZE = 16\n",
    "SPLIT_RATIO = 0.8\n",
    "TARGET_SIZE = (HEIGHT, WIDTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unzip file\n",
    "import tarfile\n",
    "\n",
    "filename = 'dataset/synthetic_sugarbeet_random_weeds'\n",
    "\n",
    "# my_tar = tarfile.open(filename + '.tar.gz')\n",
    "# my_tar.extractall('dataset')\n",
    "# my_tar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "def make_directory(fullpath):\n",
    "    splitted_data = fullpath.split('/')\n",
    "    \n",
    "    new_dir = []\n",
    "    current_dir = '.'\n",
    "    \n",
    "    for folder in splitted_data:           \n",
    "        current_dir = current_dir + '/' + folder                \n",
    "        if not os.path.isdir(current_dir + '/'):\n",
    "            new_dir.append(current_dir)\n",
    "            os.mkdir(current_dir)\n",
    "    \n",
    "    return new_dir\n",
    "    \n",
    "print(make_directory('dataset/synthetic_sugarbeet_random_weeds/tom/fougere/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a python function called split_data which takes\n",
    "# a FOLDER_IMAGES directory containing the images\n",
    "# a FOLDER_MASKS directory containing the masks\n",
    "# a TARGET_FOLDER directory where the files will be copied to\n",
    "# the TARGET_FOLDER directory will contain 2 subfolders train and test with the data splitted\n",
    "# a SPLIT SIZE to determine the portion\n",
    "# The files should also be randomized, so that the training set is a random\n",
    "# X% of the files, and the test set is the remaining files\n",
    "# SO, for example, if SPLIT SIZE is .9\n",
    "# Then 90% of the images will be copied to the TRAINING dir\n",
    "# and 10% of the images will be copied to the TESTING dir\n",
    "# Also -- All images should be checked, and if they have a zero file length,\n",
    "# they will not be copied over\n",
    "def split_data(FOLDER_IMAGES, FOLDER_MASKS, TARGET_FOLDER, SPLIT_SIZE):\n",
    "\n",
    "    TRAINING_FOLDER_NAME = TARGET_FOLDER + '/train/'\n",
    "    TESTING_FOLDER_NAME = TARGET_FOLDER + '/test/'\n",
    "    TRAINING_FOLDER_NAME_IMAGES = TARGET_FOLDER + '/train/images/img/'\n",
    "    TRAINING_FOLDER_NAME_MASKS = TARGET_FOLDER + '/train/masks/img/'\n",
    "    TESTING_FOLDER_NAME_IMAGES = TARGET_FOLDER + '/test/images/img/'\n",
    "    TESTING_FOLDER_NAME_MASKS = TARGET_FOLDER + '/test/masks/img/'\n",
    "\n",
    "    # Create directories\n",
    "    make_directory(TRAINING_FOLDER_NAME_IMAGES)\n",
    "    make_directory(TRAINING_FOLDER_NAME_MASKS)\n",
    "    make_directory(TESTING_FOLDER_NAME_IMAGES)\n",
    "    make_directory(TESTING_FOLDER_NAME_MASKS)\n",
    "    \n",
    "    # Remove all data in TRAINING and TESTING dir\n",
    "    for i_file in os.listdir(TRAINING_FOLDER_NAME_IMAGES):\n",
    "        os.remove(TRAINING_FOLDER_NAME_IMAGES + i_file)\n",
    "    for i_file in os.listdir(TRAINING_FOLDER_NAME_MASKS):\n",
    "        os.remove(TRAINING_FOLDER_NAME_MASKS + i_file)\n",
    "    for i_file in os.listdir(TESTING_FOLDER_NAME_IMAGES):\n",
    "        os.remove(TESTING_FOLDER_NAME_IMAGES + i_file)\n",
    "    for i_file in os.listdir(TESTING_FOLDER_NAME_MASKS):\n",
    "        os.remove(TESTING_FOLDER_NAME_MASKS + i_file)\n",
    "    \n",
    "    dataset = []\n",
    "    \n",
    "    # Check for z zero file length\n",
    "    for i_file in os.listdir(FOLDER_IMAGES):\n",
    "        data = i_file\n",
    "        if (os.path.getsize(FOLDER_IMAGES + data) > 0):\n",
    "            dataset.append(i_file)\n",
    "        else:\n",
    "            print('Skipped ' + i_file)\n",
    "            print('Invalid file size! i.e Zero length.')\n",
    "    \n",
    "    # Number of files\n",
    "    nb_files = len(dataset)\n",
    "    nb_files_training = int(nb_files * SPLIT_SIZE)\n",
    "    nb_files_testing = nb_files - nb_files_training\n",
    "    \n",
    "    # Suffle dataset\n",
    "    shuffled_dataset = random.sample(dataset, len(dataset))\n",
    "        \n",
    "    # Copy files\n",
    "    for i_num, i_file in enumerate(shuffled_dataset):\n",
    "        if i_num < nb_files_training:\n",
    "            new_path_images = TRAINING_FOLDER_NAME_IMAGES + i_file\n",
    "            new_path_masks = TRAINING_FOLDER_NAME_MASKS + i_file\n",
    "        else:\n",
    "            new_path_images = TESTING_FOLDER_NAME_IMAGES + i_file\n",
    "            new_path_masks = TESTING_FOLDER_NAME_MASKS + i_file\n",
    "            \n",
    "        copyfile(FOLDER_IMAGES + i_file, new_path_images)\n",
    "        copyfile(FOLDER_MASKS + i_file, new_path_masks)\n",
    "        \n",
    "    return nb_files_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "FOLDER_IMAGES = \"dataset/synthetic_sugarbeet_random_weeds/rgb/\"\n",
    "FOLDER_MASKS = \"dataset/synthetic_sugarbeet_random_weeds/gt/\"\n",
    "TARGET_FOLDER = \"dataset/synthetic_sugarbeet_random_weeds/train_test/\"\n",
    "\n",
    "NB_TRAINED_IMAGES = split_data(FOLDER_IMAGES, FOLDER_MASKS, TARGET_FOLDER, SPLIT_RATIO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trained images: 626\n",
      "(360, 480, 3)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot image\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pathname = 'train/masks/img/'\n",
    "\n",
    "list_files = os.listdir(TARGET_FOLDER + pathname)\n",
    "nb_files = len(list_files)\n",
    "print('Number of trained images:', nb_files)\n",
    " \n",
    "img_example = cv2.imread(TARGET_FOLDER + pathname + list_files[1])\n",
    "# cv2.imshow('Example', img_example) \n",
    "# cv2.waitKey(0)\n",
    "\n",
    "print(img_example.shape)\n",
    "\n",
    "plt.hist(img_example[:,:,0].ravel(), 256, [0,10]);\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_reshape(img):\n",
    "    return img[:,:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 626 images belonging to 1 classes.\n",
      "Found 626 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "# Data augmentation (for train dataset)\n",
    "SEED = 1\n",
    "\n",
    "train_image_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale = 1/255.,\n",
    "                                                                horizontal_flip=True,\n",
    "                                                                rotation_range=20,\n",
    "                                                                zoom_range=0.2,\n",
    "                                                                width_shift_range = 0.1,\n",
    "                                                                height_shift_range = 0.1)\n",
    "train_image_generator= train_image_datagen.flow_from_directory(TARGET_FOLDER + 'train/images',\n",
    "                                                               batch_size=BATCH_SIZE,\n",
    "                                                               target_size=TARGET_SIZE,\n",
    "                                                               class_mode=None,\n",
    "                                                               seed=SEED)\n",
    "\n",
    "train_mask_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "                                                                horizontal_flip=True,\n",
    "                                                                rotation_range=20,\n",
    "                                                                zoom_range=0.2,\n",
    "                                                                width_shift_range = 0.1,\n",
    "                                                                height_shift_range = 0.1,\n",
    "                                                                preprocessing_function=preprocessing_reshape)\n",
    "train_mask_generator= train_image_datagen.flow_from_directory(TARGET_FOLDER + 'train/masks/',\n",
    "                                                               batch_size=BATCH_SIZE,\n",
    "                                                               target_size=TARGET_SIZE,\n",
    "                                                               class_mode=None,\n",
    "                                                               seed=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 626 images belonging to 1 classes.\n",
      "Found 626 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "# Data augmentation (for validation dataset)\n",
    "SEED = 1\n",
    "\n",
    "val_image_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale = 1/255.)\n",
    "val_image_generator= val_image_datagen.flow_from_directory(TARGET_FOLDER + 'test/images',\n",
    "                                                               batch_size=BATCH_SIZE,\n",
    "                                                               target_size=TARGET_SIZE,\n",
    "                                                               class_mode=None,\n",
    "                                                               seed=SEED)\n",
    "\n",
    "val_mask_datagen = tf.keras.preprocessing.image.ImageDataGenerator(preprocessing_function=preprocessing_reshape)\n",
    "val_mask_generator= val_image_datagen.flow_from_directory(TARGET_FOLDER + 'test/masks/',\n",
    "                                                               batch_size=BATCH_SIZE,\n",
    "                                                               target_size=TARGET_SIZE,\n",
    "                                                               class_mode=None,\n",
    "                                                               seed=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create generator\n",
    "def my_image_mask_generator(image_data_generator, mask_data_generator):\n",
    "    new_generator = zip(image_data_generator, mask_data_generator)\n",
    "    for (img, mask) in new_generator:\n",
    "        yield (img, mask, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameter describing where the channel dimension is found in our dataset\n",
    "IMAGE_ORDERING = 'channels_last'\n",
    "\n",
    "def conv_block(input, filters, strides, pooling_size, pool_strides):\n",
    "  '''\n",
    "  Args:\n",
    "    input (tensor) -- batch of images or features\n",
    "    filters (int) -- number of filters of the Conv2D layers\n",
    "    strides (int) -- strides setting of the Conv2D layers\n",
    "    pooling_size (int) -- pooling size of the MaxPooling2D layers\n",
    "    pool_strides (int) -- strides setting of the MaxPooling2D layers\n",
    "  \n",
    "  Returns:\n",
    "    (tensor) max pooled and batch-normalized features of the input \n",
    "  '''\n",
    "  # use the functional syntax to stack the layers as shown in the diagram above\n",
    "  x = tf.keras.layers.Conv2D(filters, strides, padding='same', data_format=IMAGE_ORDERING)(input)\n",
    "  x= tf.keras.layers.LeakyReLU()(x)\n",
    "  x = tf.keras.layers.Conv2D(filters, strides, padding='same')(x)\n",
    "  x = tf.keras.layers.LeakyReLU()(x)\n",
    "  x = tf.keras.layers.MaxPooling2D(pool_size=pooling_size, strides=pool_strides)(x)\n",
    "  x = tf.keras.layers.BatchNormalization()(x)\n",
    "\n",
    "  return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_5 (InputLayer)         [(None, 64, 64, 3)]       0         \n",
      "_________________________________________________________________\n",
      "conv2d_38 (Conv2D)           (None, 64, 64, 32)        896       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_32 (LeakyReLU)   (None, 64, 64, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_39 (Conv2D)           (None, 64, 64, 32)        9248      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_33 (LeakyReLU)   (None, 64, 64, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_16 (MaxPooling (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_16 (Batc (None, 32, 32, 32)        128       \n",
      "=================================================================\n",
      "Total params: 10,272\n",
      "Trainable params: 10,208\n",
      "Non-trainable params: 64\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# TEST CODE:\n",
    "\n",
    "test_input = tf.keras.layers.Input(shape=(HEIGHT, WIDTH, 3))\n",
    "test_output = conv_block(test_input, 32, 3, 2, 2)\n",
    "test_model = tf.keras.Model(inputs=test_input, outputs=test_output)\n",
    "\n",
    "print(test_model.summary())\n",
    "\n",
    "# free up test resources\n",
    "del test_input, test_output, test_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FCN8(input_height=HEIGHT, input_width=WIDTH):\n",
    "    '''\n",
    "    Defines the downsampling path of the image segmentation model.\n",
    "\n",
    "    Args:\n",
    "      input_height (int) -- height of the images\n",
    "      width (int) -- width of the images\n",
    "\n",
    "    Returns:\n",
    "    (tuple of tensors, tensor)\n",
    "      tuple of tensors -- features extracted at blocks 3 to 5\n",
    "      tensor -- copy of the input\n",
    "    '''\n",
    "   \n",
    "    img_input = tf.keras.layers.Input(shape=(input_height,input_width, 3))\n",
    "    \n",
    "    # pad the input image to have dimensions to the nearest power of two\n",
    "    x = tf.keras.layers.ZeroPadding2D(padding=(0, 0))(img_input)\n",
    "\n",
    "    # Block 1\n",
    "    x = conv_block(x, filters=32, strides=(3, 3), pooling_size=(2, 2), pool_strides=(2, 2))\n",
    "    \n",
    "    # Block 2\n",
    "    x = conv_block(x, filters=64, strides=(3, 3), pooling_size=(2, 2), pool_strides=(2, 2))\n",
    "\n",
    "    # Block 3\n",
    "    x = conv_block(x, filters=128, strides=(3, 3), pooling_size=(2, 2), pool_strides=(2, 2))\n",
    "    # save the feature map at this stage\n",
    "    f3 = x\n",
    "\n",
    "    # Block 4\n",
    "    x = conv_block(x, filters=256, strides=(3, 3), pooling_size=(2, 2), pool_strides=(2, 2))\n",
    "    # save the feature map at this stage\n",
    "    f4 = x\n",
    "\n",
    "    # Block 5\n",
    "    x = conv_block(x, filters=256, strides=(3, 3), pooling_size=(2, 2), pool_strides=(2, 2))\n",
    "    # save the feature map at this stage\n",
    "    f5 = x\n",
    "  \n",
    "    return (f3, f4, f5), img_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_6 (InputLayer)         [(None, 64, 64, 3)]       0         \n",
      "_________________________________________________________________\n",
      "zero_padding2d_3 (ZeroPaddin (None, 64, 64, 3)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_40 (Conv2D)           (None, 64, 64, 32)        896       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_34 (LeakyReLU)   (None, 64, 64, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_41 (Conv2D)           (None, 64, 64, 32)        9248      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_35 (LeakyReLU)   (None, 64, 64, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_17 (MaxPooling (None, 32, 32, 32)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_17 (Batc (None, 32, 32, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_42 (Conv2D)           (None, 32, 32, 64)        18496     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_36 (LeakyReLU)   (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_43 (Conv2D)           (None, 32, 32, 64)        36928     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_37 (LeakyReLU)   (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_18 (MaxPooling (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_18 (Batc (None, 16, 16, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_44 (Conv2D)           (None, 16, 16, 128)       73856     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_38 (LeakyReLU)   (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_45 (Conv2D)           (None, 16, 16, 128)       147584    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_39 (LeakyReLU)   (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_19 (MaxPooling (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_19 (Batc (None, 8, 8, 128)         512       \n",
      "_________________________________________________________________\n",
      "conv2d_46 (Conv2D)           (None, 8, 8, 256)         295168    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_40 (LeakyReLU)   (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_47 (Conv2D)           (None, 8, 8, 256)         590080    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_41 (LeakyReLU)   (None, 8, 8, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_20 (MaxPooling (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_20 (Batc (None, 4, 4, 256)         1024      \n",
      "_________________________________________________________________\n",
      "conv2d_48 (Conv2D)           (None, 4, 4, 256)         590080    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_42 (LeakyReLU)   (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_49 (Conv2D)           (None, 4, 4, 256)         590080    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_43 (LeakyReLU)   (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_21 (MaxPooling (None, 2, 2, 256)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_21 (Batc (None, 2, 2, 256)         1024      \n",
      "=================================================================\n",
      "Total params: 2,355,360\n",
      "Trainable params: 2,353,888\n",
      "Non-trainable params: 1,472\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# TEST CODE:\n",
    "test_convs, test_img_input = FCN8()\n",
    "test_model = tf.keras.Model(inputs=test_img_input, outputs=[test_convs, test_img_input])\n",
    "\n",
    "print(test_model.summary())\n",
    "\n",
    "del test_convs, test_img_input, test_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fcn8_decoder(convs, n_classes):\n",
    "  # features from the encoder stage\n",
    "  f3, f4, f5 = convs\n",
    "\n",
    "  # number of filters\n",
    "  n = 512\n",
    "\n",
    "  # add convolutional layers on top of the CNN extractor.\n",
    "  o = tf.keras.layers.Conv2D(n , (7 , 7) , activation='relu' , padding='same', name=\"conv6\", data_format=IMAGE_ORDERING)(f5)\n",
    "  o = tf.keras.layers.Dropout(0.5)(o)\n",
    "\n",
    "  o = tf.keras.layers.Conv2D(n , (1 , 1) , activation='relu' , padding='same', name=\"conv7\", data_format=IMAGE_ORDERING)(o)\n",
    "  o = tf.keras.layers.Dropout(0.5)(o)\n",
    "\n",
    "  o = tf.keras.layers.Conv2D(n_classes,  (1, 1), activation='relu' , padding='same', data_format=IMAGE_ORDERING)(o)\n",
    "\n",
    "\n",
    "  # Upsample `o` above and crop any extra pixels introduced\n",
    "  o = tf.keras.layers.Conv2DTranspose(n_classes , kernel_size=(4,4) ,  strides=(2,2) , use_bias=False )(f5)\n",
    "  o = tf.keras.layers.Cropping2D(cropping=(1,1))(o)\n",
    "\n",
    "  # load the pool 4 prediction and do a 1x1 convolution to reshape it to the same shape of `o` above\n",
    "  o2 = f4\n",
    "  o2 = tf.keras.layers.Conv2D(n_classes, kernel_size=(1, 1), activation='relu', padding='same', data_format=IMAGE_ORDERING)(o2)\n",
    "\n",
    "  # add the results of the upsampling and pool 4 prediction\n",
    "  o = tf.keras.layers.Add()([o, o2])\n",
    "\n",
    "  # upsample the resulting tensor of the operation you just did\n",
    "  o = tf.keras.layers.Conv2DTranspose(n_classes , kernel_size=(4,4) ,  strides=(2,2) , use_bias=False )(o)\n",
    "  o = tf.keras.layers.Cropping2D(cropping=(1,1))(o)\n",
    "\n",
    "  # load the pool 3 prediction and do a 1x1 convolution to reshape it to the same shape of `o` above\n",
    "  o2 = f3\n",
    "  o2 = tf.keras.layers.Conv2D(n_classes , ( 1 , 1 ) , activation='relu' , padding='same', data_format=IMAGE_ORDERING)(o2)\n",
    "\n",
    "  # add the results of the upsampling and pool 3 prediction\n",
    "  o = tf.keras.layers.Add()([o, o2])\n",
    "\n",
    "  # upsample up to the size of the original image\n",
    "  o = tf.keras.layers.Conv2DTranspose(n_classes , kernel_size=(8,8) ,  strides=(8,8) , use_bias=False )(o)\n",
    "  o = tf.keras.layers.Cropping2D(((0, HEIGHT-HEIGHT), (0, 0)))(o)\n",
    "\n",
    "  # append a sigmoid activation\n",
    "  o = (tf.keras.layers.Activation('sigmoid'))(o)\n",
    "\n",
    "  return o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 64, 64, 1)\n"
     ]
    }
   ],
   "source": [
    "# TEST CODE:\n",
    "test_convs, test_img_input = FCN8()\n",
    "test_fcn8_decoder = fcn8_decoder(test_convs, 1)\n",
    "\n",
    "print(test_fcn8_decoder.shape)\n",
    "\n",
    "del test_convs, test_img_input, test_fcn8_decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 64, 64, 3)\n"
     ]
    }
   ],
   "source": [
    "# start the encoder using the default input size 64 x 84\n",
    "convs, img_input = FCN8()\n",
    "\n",
    "# pass the convolutions obtained in the encoder to the decoder\n",
    "n_classes = 3\n",
    "dec_op = fcn8_decoder(convs, n_classes)\n",
    "\n",
    "print(dec_op.shape)\n",
    "\n",
    "# define the model specifying the input (batch of images) and output (decoder output)\n",
    "model = tf.keras.Model(inputs = img_input, outputs = dec_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 100.0 steps\n",
      "Epoch 1/10\n",
      "100/100 [==============================] - 29s 291ms/step - loss: 0.4518 - accuracy: 0.7465\n",
      "Epoch 2/10\n",
      "100/100 [==============================] - 25s 249ms/step - loss: 0.3355 - accuracy: 0.8083\n",
      "Epoch 3/10\n",
      "100/100 [==============================] - 26s 255ms/step - loss: 0.2257 - accuracy: 0.8665\n",
      "Epoch 4/10\n",
      "100/100 [==============================] - 24s 242ms/step - loss: 0.1359 - accuracy: 0.9107\n",
      "Epoch 5/10\n",
      "100/100 [==============================] - 25s 251ms/step - loss: 0.0745 - accuracy: 0.9414\n",
      "Epoch 6/10\n",
      "100/100 [==============================] - 25s 251ms/step - loss: 0.0399 - accuracy: 0.9586\n",
      "Epoch 7/10\n",
      "100/100 [==============================] - 25s 245ms/step - loss: 0.0232 - accuracy: 0.9615\n",
      "Epoch 8/10\n",
      "100/100 [==============================] - 25s 248ms/step - loss: 0.0150 - accuracy: 0.9601\n",
      "Epoch 9/10\n",
      "100/100 [==============================] - 24s 243ms/step - loss: 0.0105 - accuracy: 0.9613\n",
      "Epoch 10/10\n",
      "100/100 [==============================] - 25s 250ms/step - loss: 0.0078 - accuracy: 0.9622\n"
     ]
    }
   ],
   "source": [
    "# Create custom generator for training images and masks\n",
    "my_train_generator = my_image_mask_generator(train_image_generator, train_mask_generator)\n",
    "my_val_generator = my_image_mask_generator(val_image_generator, val_mask_generator)\n",
    "\n",
    "# Compile your model here\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(lr = 1e-4),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "steps = NB_TRAINED_IMAGES / BATCH_SIZE\n",
    "\n",
    "# Train your model here\n",
    "history = model.fit(my_train_generator,\n",
    "                    steps_per_epoch=steps,\n",
    "                    epochs=10,\n",
    "                    verbose=1,\n",
    "                    validation_data=my_val_generator,\n",
    "                    validation_steps=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
