{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import random\n",
    "from shutil import copyfile\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from ae_models import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "HEIGHT = 64\n",
    "WIDTH = 64\n",
    "BATCH_SIZE = 32\n",
    "SPLIT_RATIO = 0.9\n",
    "TARGET_SIZE = (HEIGHT, WIDTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unzip file\n",
    "import tarfile\n",
    "\n",
    "filename = 'dataset/synthetic_sugarbeet_random_weeds'\n",
    "\n",
    "# my_tar = tarfile.open(filename + '.tar.gz')\n",
    "# my_tar.extractall('dataset')\n",
    "# my_tar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create all directories from a string path\n",
    "def make_directory(fullpath):\n",
    "    splitted_data = fullpath.split('/')\n",
    "    \n",
    "    new_dir = []\n",
    "    current_dir = '.'\n",
    "    \n",
    "    for folder in splitted_data:           \n",
    "        current_dir = current_dir + '/' + folder                \n",
    "        if not os.path.isdir(current_dir + '/'):\n",
    "            new_dir.append(current_dir)\n",
    "            os.mkdir(current_dir)\n",
    "    \n",
    "    return new_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a python function called split_data which takes\n",
    "# a FOLDER_IMAGES directory containing the images\n",
    "# a FOLDER_MASKS directory containing the masks\n",
    "# a TARGET_FOLDER directory where the files will be copied to\n",
    "# the TARGET_FOLDER directory will contain 2 subfolders train and test with the data splitted\n",
    "# a SPLIT SIZE to determine the portion\n",
    "# The files should also be randomized, so that the training set is a random\n",
    "# X% of the files, and the test set is the remaining files\n",
    "# SO, for example, if SPLIT SIZE is .9\n",
    "# Then 90% of the images will be copied to the TARGET_FOLDER/train dir\n",
    "# and 10% of the images will be copied to the TARGET_FOLDER/test dir\n",
    "# Also -- All images should be checked, and if they have a zero file length,\n",
    "# they will not be copied over\n",
    "def split_data(FOLDER_IMAGES, FOLDER_MASKS, TARGET_FOLDER, SPLIT_SIZE):\n",
    "\n",
    "    TRAINING_FOLDER_NAME = TARGET_FOLDER + '/train/'\n",
    "    TESTING_FOLDER_NAME = TARGET_FOLDER + '/test/'\n",
    "    TRAINING_FOLDER_NAME_IMAGES = TARGET_FOLDER + '/train/images/img/'\n",
    "    TRAINING_FOLDER_NAME_MASKS = TARGET_FOLDER + '/train/masks/img/'\n",
    "    TESTING_FOLDER_NAME_IMAGES = TARGET_FOLDER + '/test/images/img/'\n",
    "    TESTING_FOLDER_NAME_MASKS = TARGET_FOLDER + '/test/masks/img/'\n",
    "\n",
    "    # Create directories\n",
    "    make_directory(TRAINING_FOLDER_NAME_IMAGES)\n",
    "    make_directory(TRAINING_FOLDER_NAME_MASKS)\n",
    "    make_directory(TESTING_FOLDER_NAME_IMAGES)\n",
    "    make_directory(TESTING_FOLDER_NAME_MASKS)\n",
    "    \n",
    "    # Remove all data in TRAINING and TESTING dir\n",
    "    for i_file in os.listdir(TRAINING_FOLDER_NAME_IMAGES):\n",
    "        os.remove(TRAINING_FOLDER_NAME_IMAGES + i_file)\n",
    "    for i_file in os.listdir(TRAINING_FOLDER_NAME_MASKS):\n",
    "        os.remove(TRAINING_FOLDER_NAME_MASKS + i_file)\n",
    "    for i_file in os.listdir(TESTING_FOLDER_NAME_IMAGES):\n",
    "        os.remove(TESTING_FOLDER_NAME_IMAGES + i_file)\n",
    "    for i_file in os.listdir(TESTING_FOLDER_NAME_MASKS):\n",
    "        os.remove(TESTING_FOLDER_NAME_MASKS + i_file)\n",
    "    \n",
    "    dataset = []\n",
    "    \n",
    "    # Check for z zero file length\n",
    "    for i_file in os.listdir(FOLDER_IMAGES):\n",
    "        data = i_file\n",
    "        if (os.path.getsize(FOLDER_IMAGES + data) > 0):\n",
    "            dataset.append(i_file)\n",
    "        else:\n",
    "            print('Skipped ' + i_file)\n",
    "            print('Invalid file size! i.e Zero length.')\n",
    "    \n",
    "    # Number of files\n",
    "    nb_files = len(dataset)\n",
    "    nb_files_training = int(nb_files * SPLIT_SIZE)\n",
    "    nb_files_testing = nb_files - nb_files_training\n",
    "    \n",
    "    # Suffle dataset\n",
    "    shuffled_dataset = random.sample(dataset, len(dataset))\n",
    "        \n",
    "    # Copy files\n",
    "    for i_num, i_file in enumerate(shuffled_dataset):\n",
    "        if i_num < nb_files_training:\n",
    "            new_path_images = TRAINING_FOLDER_NAME_IMAGES + i_file\n",
    "            new_path_masks = TRAINING_FOLDER_NAME_MASKS + i_file\n",
    "        else:\n",
    "            new_path_images = TESTING_FOLDER_NAME_IMAGES + i_file\n",
    "            new_path_masks = TESTING_FOLDER_NAME_MASKS + i_file\n",
    "            \n",
    "        copyfile(FOLDER_IMAGES + i_file, new_path_images)\n",
    "        copyfile(FOLDER_MASKS + i_file, new_path_masks)\n",
    "        \n",
    "    return nb_files_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trained images: 1126\n"
     ]
    }
   ],
   "source": [
    "# Split data\n",
    "FOLDER_IMAGES = \"dataset/synthetic_sugarbeet_random_weeds/rgb/\"\n",
    "FOLDER_MASKS = \"dataset/synthetic_sugarbeet_random_weeds/gt/\"\n",
    "TARGET_FOLDER = \"dataset/synthetic_sugarbeet_random_weeds/train_test/\"\n",
    "\n",
    "NB_TRAINED_IMAGES = split_data(FOLDER_IMAGES, FOLDER_MASKS, TARGET_FOLDER, SPLIT_RATIO)\n",
    "print('Number of trained images:', NB_TRAINED_IMAGES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0.0, 1.0, 2.0}\n",
      "{0.0, 1.0, 2.0}\n",
      "{0.0, 1.0, 2.0}\n"
     ]
    }
   ],
   "source": [
    "# Understand how the mask is made\n",
    "mask_path = \"dataset/synthetic_sugarbeet_random_weeds/gt/\"\n",
    "list_masks = os.listdir(mask_path)\n",
    "\n",
    "masks_layer1 = []\n",
    "masks_layer2 = []\n",
    "masks_layer3 = []\n",
    "for i in range(5):\n",
    "    current_mask = cv2.imread(mask_path + list_masks[i])\n",
    "    masks_layer1 = np.concatenate([masks_layer1, current_mask[:,:,0].flatten()])\n",
    "    masks_layer2 = np.concatenate([masks_layer1, current_mask[:,:,1].flatten()])\n",
    "    masks_layer3 = np.concatenate([masks_layer1, current_mask[:,:,2].flatten()])\n",
    "    \n",
    "print(set(masks_layer1))\n",
    "print(set(masks_layer2))\n",
    "print(set(masks_layer3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Plot masks\n",
    "mask_path = \"dataset/synthetic_sugarbeet_random_weeds/gt/\"\n",
    "list_masks = os.listdir(mask_path)\n",
    "\n",
    "for i in range(3):\n",
    "    current_mask = cv2.imread(mask_path + list_masks[i])\n",
    "    current_mask = current_mask*100\n",
    "    layers = np.concatenate((current_mask[:,:,0], current_mask[:,:,1], current_mask[:,:,2]), axis=1)\n",
    "    \n",
    "    if i > 0:\n",
    "        images = np.concatenate((images, layers), axis=0)\n",
    "    else:\n",
    "        images = layers            \n",
    "\n",
    "# Plot image\n",
    "cv2.imshow('Example', images) \n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-processing of images and masks\n",
    "def preprocessing_masks(mask):\n",
    "    img = mask > 0\n",
    "    img = tf.cast(img, tf.float32)\n",
    "    return img    \n",
    "\n",
    "def preprocessing_images(image):\n",
    "    img = tf.cast(image, tf.float32)\n",
    "    img = tf.image.resize(img, TARGET_SIZE)\n",
    "    img = img / 255.\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Post processing of images and masks for visualization\n",
    "def postprocessing_masks_rgb(mask, new_size=TARGET_SIZE):\n",
    "    img = mask>0\n",
    "    img = img * 255\n",
    "    img = img.astype(np.uint8)\n",
    "    img = cv2.resize(img, (new_size[1], new_size[0]))\n",
    "    return img\n",
    "\n",
    "def postprocessing_masks_prediction(mask, new_size=TARGET_SIZE):\n",
    "    img = np.zeros(shape=(mask.shape[0], mask.shape[1], 3))\n",
    "    img[:,:,0] = img[:,:,1] = img[:,:,2]= mask[:,:,0]\n",
    "    img = img * 255\n",
    "    img = img.astype(np.uint8)\n",
    "    img = cv2.resize(img, (new_size[1], new_size[0]))\n",
    "    return img\n",
    "\n",
    "def postprocessing_images(image, new_size=TARGET_SIZE):\n",
    "    img = cv2.resize(image, (new_size[1], new_size[0]))\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot an image with its mask\n",
    "def plot_image_with_mask(image, mask):    \n",
    "    mask_norm = postprocessing_masks_rgb(mask, image.shape)\n",
    "        \n",
    "    mixed_image = cv2.addWeighted(image, 0.5, mask_norm, 0.5, 0)      \n",
    "    raw_images = np.concatenate((image, mask_norm), axis=1)\n",
    "    example = np.concatenate((raw_images, mixed_image), axis=1)\n",
    "    \n",
    "    # Plot image\n",
    "    cv2.imshow('Example', example) \n",
    "    cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images: 1126\n",
      "Selected image: 0\n",
      "70\n",
      "133\n",
      "172\n"
     ]
    }
   ],
   "source": [
    "list_files = os.listdir(TARGET_FOLDER + 'train/images/img/')\n",
    "print('Number of images:', len(list_files))\n",
    "IMG_ID = 0\n",
    "print('Selected image:', IMG_ID)\n",
    "\n",
    "example_image = cv2.imread(TARGET_FOLDER + 'train/images/img/' + list_files[IMG_ID])\n",
    "example_mask = cv2.imread(TARGET_FOLDER + 'train/masks/img/' + list_files[IMG_ID])\n",
    "\n",
    "print(np.max(example_image[:,:,0]))\n",
    "print(np.max(example_image[:,:,1]))\n",
    "print(np.max(example_image[:,:,2]))\n",
    "\n",
    "plot_image_with_mask(example_image, example_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1126 images belonging to 1 classes.\n",
      "Found 1126 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "# Data augmentation (for train dataset)\n",
    "SEED = 0\n",
    "\n",
    "train_image_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale = 1/255.,\n",
    "                                                                horizontal_flip=True,\n",
    "                                                                rotation_range=20,\n",
    "                                                                zoom_range=0.2,\n",
    "                                                                width_shift_range = 0.1,\n",
    "                                                                height_shift_range = 0.1)\n",
    "train_image_generator = train_image_datagen.flow_from_directory(TARGET_FOLDER + 'train/images',\n",
    "                                                               batch_size=BATCH_SIZE,\n",
    "                                                               target_size=TARGET_SIZE,\n",
    "                                                               color_mode='rgb',\n",
    "                                                               class_mode=None,\n",
    "                                                               seed=SEED)\n",
    "\n",
    "train_mask_datagen = tf.keras.preprocessing.image.ImageDataGenerator(preprocessing_function=preprocessing_masks,\n",
    "                                                                horizontal_flip=True,\n",
    "                                                                rotation_range=20,\n",
    "                                                                zoom_range=0.2,\n",
    "                                                                width_shift_range = 0.1,\n",
    "                                                                height_shift_range = 0.1)\n",
    "train_mask_generator = train_mask_datagen.flow_from_directory(TARGET_FOLDER + 'train/masks/',\n",
    "                                                               batch_size=BATCH_SIZE,\n",
    "                                                               target_size=TARGET_SIZE,\n",
    "                                                               color_mode='grayscale',\n",
    "                                                               class_mode=None,\n",
    "                                                               seed=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 126 images belonging to 1 classes.\n",
      "Found 126 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "# Data augmentation (for validation dataset)\n",
    "SEED = 1\n",
    "\n",
    "val_image_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale = 1/255.)\n",
    "val_image_generator= val_image_datagen.flow_from_directory(TARGET_FOLDER + 'test/images',\n",
    "                                                               batch_size=BATCH_SIZE,\n",
    "                                                               target_size=TARGET_SIZE,\n",
    "                                                               class_mode=None,\n",
    "                                                               seed=SEED)\n",
    "\n",
    "val_mask_datagen = tf.keras.preprocessing.image.ImageDataGenerator(preprocessing_function=preprocessing_masks)\n",
    "val_mask_generator= val_image_datagen.flow_from_directory(TARGET_FOLDER + 'test/masks/',\n",
    "                                                               batch_size=BATCH_SIZE,\n",
    "                                                               target_size=TARGET_SIZE,\n",
    "                                                               class_mode=None,\n",
    "                                                               seed=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(360, 480, 1)\n",
      "0.007843138\n",
      "2.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create custom generator for training images and masks\n",
    "my_train_generator = zip(train_image_generator, train_mask_generator)\n",
    "my_val_generator = zip(val_image_generator, val_mask_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display one image from image generator\n",
    "\n",
    "my_train_generator.next()\n",
    "\n",
    "img_gen = train_image_generator.next()\n",
    "\n",
    "image = img_gen[0]\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "mask_gen = train_mask_generator.next()\n",
    "\n",
    "mask = np.zeros(shape=image.shape)\n",
    "mask[:,:,0] = mask[:,:,1] = mask[:,:,2] = mask_gen[0][:,:,0] * 255.\n",
    "\n",
    "print(np.max(mask))\n",
    "\n",
    "# Create concatenation\n",
    "image_mask = np.concatenate((image, mask), axis=1)\n",
    "\n",
    "# Plot image\n",
    "cv2.imshow('Example from generator', image_mask) \n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 64, 64, 1)\n"
     ]
    }
   ],
   "source": [
    "# start the encoder using the default input size 64 x 84\n",
    "convs, img_input = FCN8()\n",
    "\n",
    "# pass the convolutions obtained in the encoder to the decoder\n",
    "n_classes = 1\n",
    "dec_op = fcn8_decoder(convs, n_classes)\n",
    "\n",
    "print(dec_op.shape)\n",
    "\n",
    "# define the model specifying the input (batch of images) and output (decoder output)\n",
    "model = tf.keras.Model(inputs = img_input, outputs = dec_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps: 156.25\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 156.25 steps, validate for 156.25 steps\n",
      "Epoch 1/30\n",
      "157/156 [==============================] - 171s 1s/step - loss: 0.4943 - accuracy: 0.6638 - val_loss: 0.1432 - val_accuracy: 0.9577\n",
      "Epoch 2/30\n",
      "157/156 [==============================] - 167s 1s/step - loss: 0.0170 - accuracy: 0.9473 - val_loss: 0.0148 - val_accuracy: 0.9577\n",
      "Epoch 3/30\n",
      "157/156 [==============================] - 165s 1s/step - loss: 0.0038 - accuracy: 0.9472 - val_loss: 0.0057 - val_accuracy: 0.9577\n",
      "Epoch 4/30\n",
      "157/156 [==============================] - 163s 1s/step - loss: 0.0026 - accuracy: 0.9473 - val_loss: 0.0029 - val_accuracy: 0.9576\n",
      "Epoch 5/30\n",
      "157/156 [==============================] - 165s 1s/step - loss: 0.0022 - accuracy: 0.9473 - val_loss: 0.0023 - val_accuracy: 0.9577\n",
      "Epoch 6/30\n",
      "157/156 [==============================] - 171s 1s/step - loss: 0.0019 - accuracy: 0.9473 - val_loss: 0.0021 - val_accuracy: 0.9577\n",
      "Epoch 7/30\n",
      "157/156 [==============================] - 170s 1s/step - loss: 0.0018 - accuracy: 0.9472 - val_loss: 0.0020 - val_accuracy: 0.9577\n",
      "Epoch 8/30\n",
      "157/156 [==============================] - 171s 1s/step - loss: 0.0017 - accuracy: 0.9470 - val_loss: 0.0020 - val_accuracy: 0.9577\n",
      "Epoch 9/30\n",
      "157/156 [==============================] - 172s 1s/step - loss: 0.0016 - accuracy: 0.9470 - val_loss: 0.0018 - val_accuracy: 0.9577\n",
      "Epoch 10/30\n",
      "157/156 [==============================] - 171s 1s/step - loss: 0.0016 - accuracy: 0.9478 - val_loss: 0.0023 - val_accuracy: 0.9578\n",
      "Epoch 11/30\n",
      "157/156 [==============================] - 170s 1s/step - loss: 0.0016 - accuracy: 0.9466 - val_loss: 0.0017 - val_accuracy: 0.9577\n",
      "Epoch 12/30\n",
      "157/156 [==============================] - 170s 1s/step - loss: 0.0015 - accuracy: 0.9472 - val_loss: 0.0016 - val_accuracy: 0.9577\n",
      "Epoch 13/30\n",
      "157/156 [==============================] - 171s 1s/step - loss: 0.0015 - accuracy: 0.9468 - val_loss: 0.0016 - val_accuracy: 0.9578\n",
      "Epoch 14/30\n",
      "157/156 [==============================] - 170s 1s/step - loss: 0.0014 - accuracy: 0.9476 - val_loss: 0.0016 - val_accuracy: 0.9577\n",
      "Epoch 15/30\n",
      "157/156 [==============================] - 170s 1s/step - loss: 0.0014 - accuracy: 0.9471 - val_loss: 0.0017 - val_accuracy: 0.9577\n",
      "Epoch 16/30\n",
      "157/156 [==============================] - 172s 1s/step - loss: 0.0014 - accuracy: 0.9475 - val_loss: 0.0016 - val_accuracy: 0.9577\n",
      "Epoch 17/30\n",
      "157/156 [==============================] - 169s 1s/step - loss: 0.0014 - accuracy: 0.9466 - val_loss: 0.0016 - val_accuracy: 0.9577\n",
      "Epoch 18/30\n",
      "157/156 [==============================] - 172s 1s/step - loss: 0.0014 - accuracy: 0.9473 - val_loss: 0.0015 - val_accuracy: 0.9577\n",
      "Epoch 19/30\n",
      "157/156 [==============================] - 169s 1s/step - loss: 0.0014 - accuracy: 0.9471 - val_loss: 0.0015 - val_accuracy: 0.9577\n",
      "Epoch 20/30\n",
      "157/156 [==============================] - 169s 1s/step - loss: 0.0013 - accuracy: 0.9473 - val_loss: 0.0015 - val_accuracy: 0.9576\n",
      "Epoch 21/30\n",
      "157/156 [==============================] - 181s 1s/step - loss: 0.0014 - accuracy: 0.9469 - val_loss: 0.0015 - val_accuracy: 0.9577\n",
      "Epoch 22/30\n",
      "157/156 [==============================] - 170s 1s/step - loss: 0.0013 - accuracy: 0.9471 - val_loss: 0.0015 - val_accuracy: 0.9577\n",
      "Epoch 23/30\n",
      "157/156 [==============================] - 170s 1s/step - loss: 0.0013 - accuracy: 0.9472 - val_loss: 0.0015 - val_accuracy: 0.9577\n",
      "Epoch 24/30\n",
      "157/156 [==============================] - 170s 1s/step - loss: 0.0013 - accuracy: 0.9475 - val_loss: 0.0015 - val_accuracy: 0.9578\n",
      "Epoch 25/30\n",
      "157/156 [==============================] - 171s 1s/step - loss: 0.0013 - accuracy: 0.9469 - val_loss: 0.0015 - val_accuracy: 0.9577\n",
      "Epoch 26/30\n",
      "157/156 [==============================] - 171s 1s/step - loss: 0.0013 - accuracy: 0.9468 - val_loss: 0.0015 - val_accuracy: 0.9578\n",
      "Epoch 27/30\n",
      "157/156 [==============================] - 171s 1s/step - loss: 0.0013 - accuracy: 0.9470 - val_loss: 0.0015 - val_accuracy: 0.9577\n",
      "Epoch 28/30\n",
      "157/156 [==============================] - 169s 1s/step - loss: 0.0013 - accuracy: 0.9469 - val_loss: 0.0015 - val_accuracy: 0.9577\n",
      "Epoch 29/30\n",
      "157/156 [==============================] - 169s 1s/step - loss: 0.0013 - accuracy: 0.9471 - val_loss: 0.0014 - val_accuracy: 0.9577\n",
      "Epoch 30/30\n",
      "157/156 [==============================] - 170s 1s/step - loss: 0.0013 - accuracy: 0.9472 - val_loss: 0.0015 - val_accuracy: 0.9576\n"
     ]
    }
   ],
   "source": [
    "# Compile your model here\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "steps = NB_TRAINED_IMAGES / BATCH_SIZE\n",
    "steps = 5000 / BATCH_SIZE\n",
    "print('Steps:', steps)\n",
    "\n",
    "# Train your model here\n",
    "history = model.fit(my_train_generator,\n",
    "                    steps_per_epoch=steps,\n",
    "                    epochs=30,\n",
    "                    verbose=1,\n",
    "                    validation_data=my_val_generator,\n",
    "                    validation_steps=steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Plot prediction\n",
    "IMG_ID = 1\n",
    "pathname = 'train/images/img/'\n",
    "list_files = os.listdir(TARGET_FOLDER + pathname)\n",
    "\n",
    "example_image = cv2.imread(TARGET_FOLDER + 'train/images/img/' + list_files[IMG_ID])\n",
    "example_mask = cv2.imread(TARGET_FOLDER + 'train/masks/img/' + list_files[IMG_ID])\n",
    "\n",
    "image_processed = preprocessing_images(example_image)\n",
    "image_processed = tf.expand_dims(image_processed, axis=0)\n",
    "\n",
    "prediction = model.predict(image_processed*256)\n",
    "pred_image = np.array(tf.squeeze(prediction, axis=0))\n",
    "\n",
    "image_example_gt = np.concatenate((postprocessing_images(example_image),\n",
    "                                   postprocessing_masks_rgb(example_mask),\n",
    "                                   postprocessing_masks_prediction(pred_image)), axis=1)\n",
    "cv2.imshow('Example', image_example_gt) \n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "flatten_image = pred_image.flatten()*256\n",
    "print(max(flatten_image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
